# ğŸ“Š AnÃ¡lise Completa: Classes, ConexÃµes e CÃ³digo NÃ£o Utilizado

**Data:** 12 de Janeiro de 2026  
**Projeto:** PTBR-SLM (Small Language Model para PortuguÃªs Brasileiro)  
**Framework:** Rust + Burn 0.14  
**Escopo:** 5.064 linhas de Rust em 24 arquivos

---

## ğŸ¯ O QUE ESTAMOS FAZENDO

Este Ã© um **Small Language Model (SLM) para PortuguÃªs Brasileiro** usando arquitetura **RWKV** (Receptance Weighted Key Value).

### Objetivo Principal:
Treinar um modelo de linguagem eficiente em GPU/CUDA capaz de processar texto em portuguÃªs, com suporte a:
- âœ… TokenizaÃ§Ã£o customizada (BPE)
- âœ… Treinamento distribuÃ­do
- âœ… Fine-tuning por domÃ­nio (legal, financeiro, mÃ©dico, etc)
- âœ… GeraÃ§Ã£o de texto
- âœ… InferÃªncia incrementalizada

### Stack TecnolÃ³gico:
- **Linguagem:** Rust 2021
- **Framework ML:** Burn 0.14
- **Backend:** CUDA/WGPU/NdArray (configurÃ¡vel)
- **Tokenizer:** BPE com cache LRU
- **Dataset:** Memory-mapped (.bin)

---

## ğŸ—ï¸ ARQUITETURA GERAL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               MAIN.RS (CLI Interface)              â”‚
â”‚  15 Subcomandos: ProcessWiki, Train, Generate...  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        â”‚        â”‚              â”‚
    â–¼        â–¼        â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚TOKENIZER â”‚â”‚ DATA â”‚â”‚ MODEL  â”‚â”‚   LOGGER   â”‚
â”‚ MODULE   â”‚â”‚MODULEâ”‚â”‚MODULE  â”‚â”‚   UTILS    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ MÃ“DULO 1: TOKENIZER (`src/tokenizer/`)

### Structs Principais:

#### **1. `BPEVocab`**
```rust
pub struct BPEVocab {
    pub id_to_token: Vec<Vec<u8>>,      // ID â†’ bytes
    pub merges: Vec<(u16, u16)>,        // HistÃ³rico de merges
    pub special_tokens: HashMap<String, u16>, // [PAD], [BOS], etc
}
```
- **Usado em:** Carregamento do tokenizer
- **Status:** âœ… Ativo
- **FunÃ§Ãµes:** `new()`, `build_token_to_id()`

#### **2. `BPETokenizer`** â­ (Heavy User)
```rust
pub struct BPETokenizer {
    id_to_token: Vec<Vec<u8>>,
    token_to_id: HashMap<Vec<u8>, u16>,
    merges: Vec<(u16, u16)>,
    special_tokens: HashMap<String, u16>,
    cache: Arc<RwLock<LRUCache>>,
}
```
- **Usado em:** TODOS os comandos que precisam de tokens
- **Status:** âœ… Altamente Ativo
- **FunÃ§Ãµes:**
  - `encode(text: &str) -> Vec<u16>` - **Central**
  - `decode(tokens: &[u16]) -> String`
  - `from_file(path)` - Carrega do JSON
  - `save(path)` - Salva vocab
  - `pad_id()`, `bos_id()`, `eos_id()` - Tokens especiais

#### **3. `LRUCache`** (privado)
```rust
struct LRUCache {
    map: HashMap<String, Vec<u16>>,
    order: VecDeque<String>,
    max_size: usize,
}
```
- **Usado em:** Dentro de `BPETokenizer.encode()`
- **Status:** âœ… Ativo (otimizaÃ§Ã£o)
- **Tamanho:** 100k entries mÃ¡ximo
- **BenefÃ­cio:** ~2-3x speedup em encoding

#### **4. `BPETrainer`**
```rust
pub struct BPETrainer {
    vocab_size: usize,
    num_merges: usize,
}
```
- **Usado em:** Comando `TrainTokenizer`
- **Status:** âœ… Ativo (apenas durante training)
- **FunÃ§Ãµes:** `train(corpus)`, `save(path)`

#### **5. `PTBRNormalizer`**
```rust
pub struct PTBRNormalizer {
    // PadrÃµes de normalizaÃ§Ã£o PT-BR
}
```
- **Usado em:** PreparaÃ§Ã£o de dados
- **Status:** âš ï¸ Parcial (implementado, pouco integrado)
- **O que faz:**
  - NFD decomposition (Ã£ â†’ a + ~)
  - Lowercase
  - Whitespace normalization

### Fluxo do Tokenizer:
```
Texto bruto
    â†“
PTBRNormalizer.normalize()    [âš ï¸ Pouco usado]
    â†“
BPETokenizer.encode()         [âœ… Heavy]
    â””â”€â†’ LRUCache check        [âœ… OtimizaÃ§Ã£o]
    â””â”€â†’ pre_tokenize()
    â””â”€â†’ encode_word()
    â””â”€â†’ apply merges          [HistÃ³rico de BPE]
    â†“
Vec<u16> (token IDs)
```

---

## ğŸ“¦ MÃ“DULO 2: DATA (`src/data/`)

### Structs Principais:

#### **1. `MmapDataset`** â­ (Heavy User)
```rust
pub struct MmapDataset {
    data: Mmap,                 // Memory-mapped file
    indices: Vec<usize>,        // Ãndices de sequÃªncias
    seq_len: usize,             // Comprimento da sequÃªncia
    epoch: usize,
    num_tokens: usize,
}
```
- **Usado em:** Comandos `Train`, `Resume`, `TestModel`
- **Status:** âœ… CrÃ­tico
- **Formato:** Arquivo `.bin` com u16 tokens
- **Tamanho:** Atual = 1.2 GB
- **FunÃ§Ãµes:**
  - `from_file(path, seq_len)` - Carrega sem descomprimir (fast!)
  - `batch(start_idx, batch_size)` - Retorna batch
  - `shuffle_epoch()` - Embaralha para novo epoch

#### **2. `DataLoader`**
```rust
pub struct DataLoader<'a> {
    dataset: &'a MmapDataset,
    batch_size: usize,
    current_idx: usize,
}
```
- **Usado em:** Loop de treinamento
- **Status:** âœ… Ativo
- **FunÃ§Ãµes:**
  - `next_batch()` â†’ `(input_ids, target_ids)`
  - `num_batches()` â†’ quantidade

#### **3. `WikiStreamParser`**
```rust
pub struct WikiStreamParser {
    // Parser para Wikipedia XML.BZ2
}
```
- **Usado em:** Comando `ProcessWiki`
- **Status:** âœ… Ativo (data prep)
- **O que faz:**
  - Parse XML de Wikipedia
  - Stream parsing (economiza RAM)
  - Extrai artigos

#### **4. `WikiCleaner`** â­ (Muito usado)
```rust
pub struct WikiCleaner {
    patterns: CleanerPatterns,
}
```
- **Usado em:** `ProcessWiki`, `CleanCorpus`, `AuditCorpus`
- **Status:** âœ… Muito ativo
- **Limpezas aplicadas:**
  1. Remove templates `{{ }}`
  2. Remove HTML tags `<>`
  3. Remove wiki links `[[ ]]`
  4. Remove categorias
  5. Remove imagens
  6. + 10 regex patterns adicionais
- **FunÃ§Ãµes:** `clean(text)` â†’ String limpo

#### **5. `TokenizedDatasetWriter`**
```rust
pub struct TokenizedDatasetWriter {
    writer: BufWriter<File>,
}
```
- **Usado em:** Comando `Tokenize`, `BuildDataset`
- **Status:** âœ… Ativo
- **Formato:** Escreve u16 em binÃ¡rio
- **FunÃ§Ãµes:** `write_token(id)`, `flush()`

#### **6. `CleanerPatterns`** (privado)
```rust
struct CleanerPatterns {
    template_re: Regex,
    html_re: Regex,
    link_re: Regex,
    // ... 10+ mais patterns
}
```
- **Status:** âœ… Ativo (interno)

### Fluxo de Dados:
```
Wikipedia XML.BZ2
    â†“
WikiStreamParser.parse()
    â†“
WikiCleaner.clean()           [âœ… Ativo]
    â”œâ”€ Remove templates
    â”œâ”€ Remove HTML
    â””â”€ Remove wiki markup
    â†“
BPETokenizer.encode()         [âœ… Ativo]
    â†“
TokenizedDatasetWriter        [âœ… Ativo]
    â†“
train.bin (1.2 GB)
    â†“
MmapDataset.from_file()       [âœ… Ativo]
    â†“
DataLoader.next_batch()       [âœ… Ativo]
    â†“
Trainer
```

---

## ğŸ“¦ MÃ“DULO 3: MODEL (`src/model/`) - â­ CORE

### âœ… Structs Ativos (CrÃ­ticos):

#### **1. `RWKV`** â­â­â­ (Core Model)
```rust
pub struct RWKV<B: Backend> {
    embedding: Embedding<B>,
    ln_pre: LayerNorm<B>,
    blocks: Vec<RWKVBlock<B>>,    // N layers
    ln_out: LayerNorm<B>,
    head: Linear<B>,              // vocab_size output
    
    vocab_size: usize,
    d_model: usize,
    n_layers: usize,
}
```
- **Usado em:** TODOS os comandos que usam modelo
- **Status:** âœ… CrÃ­tico
- **FunÃ§Ãµes:**
  - `forward(input_ids)` â†’ logits [batch, seq, vocab]
  - `forward_inference(input_ids)` â†’ logits [batch, vocab] (last token)
- **HiperparÃ¢metros:**
  - vocab_size: 32.000
  - d_model: 512, 1024, 2048 (configurable)
  - n_layers: 12-24 (configurable)

#### **2. `RWKVBlock`**
```rust
pub struct RWKVBlock<B: Backend> {
    ln1: LayerNorm<B>,
    time_mixing: TimeMixing<B>,    // Attention RWKV
    ln2: LayerNorm<B>,
    channel_mixing: ChannelMixing<B>, // FFN
    dropout: Dropout,
}
```
- **Usado em:** Dentro de `RWKV.forward()`
- **Status:** âœ… CrÃ­tico
- **Arquitetura:** Pre-norm + residual
  ```
  x â†’ LayerNorm â†’ TimeMixing â†’ + residual
    â†’ LayerNorm â†’ ChannelMixing â†’ + residual
  ```

#### **3. `TimeMixing`** â­ (RWKV Innovation)
```rust
pub struct TimeMixing<B: Backend> {
    receptance: Linear,
    key: Linear,
    value: Linear,
    output: Linear,
    
    time_decay: Param<Tensor>,      // WKV decay
    time_first: Param<Tensor>,      // Initial value
    time_mix_k: Param<Tensor>,      // Mix ratios
    time_mix_v: Param<Tensor>,
    time_mix_r: Param<Tensor>,
}
```
- **Usado em:** Dentro de `RWKVBlock`
- **Status:** âœ… CrÃ­tico
- **O que Ã©:** AtenÃ§Ã£o RWKV (Receptance Weighted Key-Value)
- **Complexity:** O(n) ao invÃ©s de O(nÂ²) como transformer
- **InicializaÃ§Ã£o:** Layer-dependente (camadas profundas = decay lento)

#### **4. `ChannelMixing`**
```rust
pub struct ChannelMixing<B: Backend> {
    value: Linear,
    gate: Linear,
    
    time_mix_g: Param<Tensor>,
    time_mix_k: Param<Tensor>,
}
```
- **Usado em:** Dentro de `RWKVBlock`
- **Status:** âœ… CrÃ­tico
- **O que Ã©:** FFN com gate (similar a GLU)

#### **5. `RWKVConfig`** (ConfiguraÃ§Ã£o)
```rust
pub struct RWKVConfig {
    pub vocab_size: usize,          // 32000
    pub d_model: usize,             // 512, 1024, 2048
    pub n_layers: usize,            // 12, 18, 24
    pub d_ffn: usize,               // 4 * d_model
    pub dropout: f32,               // 0.1
    pub layer_norm_eps: f32,        // 1e-5
}
```
- **Usado em:** CriaÃ§Ã£o de modelo
- **Status:** âœ… Ativo

#### **6. `Trainer`** â­â­ (Training Loop)
```rust
pub struct Trainer<B: AutodiffBackend> {
    pub model: RWKV<B>,
    optimizer: OptimizerAdaptor<AdamW>,
    config: TrainingConfig,
    
    step: usize,
    micro_step: usize,
    accumulated_loss: f32,
    
    last_grad_norm: f32,
    ema_loss: f32,              // Exponential moving average
    best_loss: f32,
    
    device: B::Device,
}
```
- **Usado em:** Comandos `Train`, `Resume`
- **Status:** âœ… CrÃ­tico
- **FunÃ§Ãµes:**
  - `train_step(input_ids, target_ids)` â†’ Option<TrainStats>
  - `validation_step(input_ids, target_ids)` â†’ f32 (loss)
  - `save_checkpoint(path)`
  - `load_checkpoint(path)`
- **Features:**
  - âœ… Gradient accumulation
  - âœ… LR schedule (cosine + warmup)
  - âœ… NaN/Inf detection
  - âœ… EMA loss tracking

#### **7. `TrainingConfig`**
```rust
pub struct TrainingConfig {
    pub learning_rate: f64,
    pub weight_decay: f64,
    pub batch_size: usize,
    pub gradient_accumulation_steps: usize,
    pub warmup_steps: usize,
    pub total_steps: usize,
    pub eval_interval: usize,
    pub save_interval: usize,
}
```
- **Status:** âœ… Ativo
- **PadrÃ£o:** LR=3e-4, WD=0.01, warmup=500

#### **8. `TrainStats`**
```rust
pub struct TrainStats {
    pub loss: f32,
    pub grad_norm: f32,
    pub lr: f64,
    pub tokens_per_sec: f32,
}
```
- **Status:** âœ… Ativo
- **Onde:** Retornado por `Trainer.train_step()`

---

### âš ï¸ Structs Parcialmente Utilizados:

#### **1. `RWKVState`**
```rust
pub struct RWKVState<B: Backend> {
    pub time_state: Vec<(Tensor, Tensor, Tensor)>,
    pub channel_state: Vec<Tensor>,
}
```
- **PropÃ³sito:** InferÃªncia incremental (gerar token por token)
- **Status:** âš ï¸ **NÃƒO ATIVO** no treinamento
- **Onde Ã© criado:** Em alguns contextos de geraÃ§Ã£o
- **Problema:** Nunca Ã© alimentado durante inference loop
- **Impacto:** GeraÃ§Ã£o atual faz forward completo a cada token

---

### âŒ Structs Nunca Utilizados:

#### **1. `CheckpointedActivation`**
```rust
pub struct CheckpointedActivation<B: Backend> {
    pub x: Tensor<B, 3>,
    pub y: Tensor<B, 3>,
}
```
- **PropÃ³sito:** Gradient checkpointing (economizar RAM)
- **Status:** âŒ **NUNCA USADO**
- **LocalizaÃ§Ã£o:** `src/model/checkpoint.rs`
- **Por quÃª:** Implementado mas nunca integrado ao forward pass

#### **2. `CheckpointConfig`**
```rust
pub struct CheckpointConfig {
    pub enabled: bool,
    pub checkpoint_segments: usize,
    pub use_recompute: bool,
}
```
- **Status:** âŒ **NUNCA USADO**
- **Impacto:** High-memory training nÃ£o otimizado

#### **3. `Evaluator`** e `EvalMetrics`
```rust
pub struct Evaluator {
    // Calcula mÃ©tricas de validaÃ§Ã£o
}

pub struct EvalMetrics {
    pub perplexity: f32,
    pub loss: f32,
    pub accuracy: f32,
}
```
- **Status:** âŒ **NUNCA USADO**
- **ImplementaÃ§Ã£o:** Completa em `src/model/evaluator.rs`
- **Problema:** `Trainer.validation_step()` existe mas nunca chamado
- **Impacto:** Sem mÃ©tricas de validaÃ§Ã£o proper durante training

#### **4. `LoRAAdapter`** âš ï¸âš ï¸âš ï¸
```rust
pub struct LoRAAdapter<B: Backend> {
    down: Linear<B>,    // d_model â†’ rank
    up: Linear<B>,      // rank â†’ d_model
    
    scale: f32,
    rank: usize,
    d_model: usize,
}
```
- **PropÃ³sito:** Fine-tuning eficiente (LoRA)
- **Status:** âŒ **NUNCA USADO**
- **LocalizaÃ§Ã£o:** `src/model/adapters.rs`
- **FunÃ§Ãµes:** `forward()`, `apply()`, `num_parameters()`
- **Problema:** Infraestrutura pronta, mas:
  - Nenhum subcomando de fine-tuning
  - Nunca criado/instanciado
  - NÃ£o integrado ao forward pass do modelo
- **ParÃ¢metros:** 2 Ã— d_model Ã— rank (ex: 2 Ã— 1024 Ã— 16 = 32k params)

#### **5. `DomainAdapterBank`** âš ï¸âš ï¸âš ï¸
```rust
pub struct DomainAdapterBank<B: Backend> {
    adapters: Vec<LoRAAdapter<B>>,
    
    active_idx: Option<usize>,
    num_adapters: usize,
}
```
- **PropÃ³sito:** Multi-domain adapters (Legal, Financial, Medical, etc)
- **Status:** âŒ **NUNCA USADO**
- **DomÃ­nios definidos:** 7 (General, Legal, Financial, Tech, Medical, Academic, News)
- **FunÃ§Ãµes:** `add_adapter()`, `set_active()`, `forward()`
- **Problema:** NÃ£o integrado em nenhum lugar

#### **6. `DomainRegistry`**
```rust
pub struct DomainRegistry {
    domains: Vec<Domain>,
}
```
- **Status:** âŒ **NUNCA USADO**
- **PropÃ³sito:** Rastrear domÃ­nios
- **Problema:** Separado de `DomainAdapterBank` para evitar bugs do Burn macro

#### **7. `DomainFineTuneConfig`**
```rust
pub struct DomainFineTuneConfig {
    pub domain: Domain,
    pub lora_rank: usize,
    pub lora_scale: f32,
    pub learning_rate: f64,
    pub weight_decay: f64,
    pub epochs: usize,
    pub batch_size: usize,
    pub gradient_accumulation: usize,
    pub freeze_base: bool,
    pub warmup_ratio: f32,
}
```
- **Status:** âŒ **NUNCA USADO**
- **Implementado para:** Legal, Financial, Tech, Medical, Academic, News
- **Problema:** Configs prontas mas nunca instanciadas

#### **8. `GradScaler`** (Mixed Precision)
```rust
pub struct GradScaler {
    scale: f32,
    backoff_factor: f32,
    growth_factor: f32,
}
```
- **Status:** âš ï¸ **NUNCA USADO**
- **PropÃ³sito:** Mixed precision (FP16 + FP32)
- **Problema:** Implementado mas nÃ£o integrado ao `Trainer`

#### **9. `LRFinderResult`**
```rust
pub struct LRFinderResult {
    pub learning_rates: Vec<f64>,
    pub losses: Vec<f32>,
    pub suggested_lr: f64,
}
```
- **Status:** âŒ **NUNCA USADO**
- **Comando:** `FindLr` estÃ¡ definido mas nunca chamado
- **LocalizaÃ§Ã£o:** `src/model/lr_finder.rs`

---

## ğŸ“¦ MÃ“DULO 4: LOGGER (`src/logger/`)

| Struct | Status | Usado? |
|--------|--------|--------|
| `TrainLogger` | âœ… | âœ… Sim (logging de treino) |
| `MetricsCSV` | âœ… | âœ… Sim (salva metrics.csv) |
| `TrainingStats` | âœ… | âœ… Sim (agregaÃ§Ã£o) |

---

## ğŸ“‹ TODOS OS 15 COMANDOS CLI

### âœ… Completamente Funcional:

| # | Comando | O que faz | Conecta |
|---|---------|----------|---------|
| 1 | `ProcessWiki` | Parse Wikipedia BZ2 | WikiStreamParser â†’ WikiCleaner |
| 2 | `TrainTokenizer` | Treina tokenizer BPE | BPETrainer â†’ BPEVocab |
| 3 | `Tokenize` | Tokeniza corpus | BPETokenizer â†’ TokenizedDatasetWriter |
| 4 | `Train` | Treina modelo | MmapDataset â†’ Trainer â†’ RWKV |
| 5 | `Resume` | Continua treino | Load checkpoint â†’ Trainer |
| 6 | `TestModel` | Testa inference | Load model â†’ RWKV.forward() |
| 7 | `Generate` | Gera texto | RWKV + sampling (temperature, top-k) |
| 8 | `CleanCorpus` | Limpa dados | WikiCleaner |
| 9 | `AuditCorpus` | Audita qualidade | Analysis |
| 10 | `Info` | Mostra config | Print RWKVConfig |
| 11 | `BuildDataset` | Build dataset | TokenizedDatasetWriter |
| 12 | `TestGpu` | Testa GPU | Aloca tensors |

### âš ï¸ Parcialmente Implementado:

| # | Comando | Status | Problema |
|---|---------|--------|----------|
| 13 | `FindLr` | âœ… CÃ³digo pronto | âŒ Nunca chamado em main() |
| 14 | `Benchmark` | âœ… CÃ³digo pronto | âŒ Nunca chamado em main() |

---

## ğŸ”— FLUXO COMPLETO: Como Tudo se Conecta

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STARTUP (main.rs)                        â”‚
â”‚  Parse CLI â†’ Match Commands â†’ Dispatch to handler           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚           â”‚           â”‚
     â–¼           â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATA    â”‚ â”‚ TRAIN  â”‚ â”‚ GENERATE â”‚
â”‚ PREP    â”‚ â”‚ PHASE  â”‚ â”‚ PHASE    â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚          â”‚            â”‚
     â”‚          â”‚            â”‚
     â–¼          â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DATA PREPARATION                           â”‚
â”‚  ProcessWiki â†’ WikiCleaner â†’ BPETokenizer â†’ Binary .bin    â”‚
â”‚                                     â–²                       â”‚
â”‚                  PTBRNormalizer â”€â”€â”€â”€â”˜                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  MmapDataset        â”‚
            â”‚  (train.bin 1.2GB)  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  DataLoader         â”‚
            â”‚  next_batch()       â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         TRAINING LOOP                â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚  Trainer.train_step()         â”‚  â”‚
    â”‚  â”‚  â”œâ”€ RWKV.forward()            â”‚  â”‚
    â”‚  â”‚  â”‚  â”œâ”€ RWKVBlock[0..N]       â”‚  â”‚
    â”‚  â”‚  â”‚  â”‚  â”œâ”€ TimeMixing         â”‚  â”‚
    â”‚  â”‚  â”‚  â”‚  â””â”€ ChannelMixing      â”‚  â”‚
    â”‚  â”‚  â”œâ”€ Cross-entropy loss       â”‚  â”‚
    â”‚  â”‚  â”œâ”€ Backward (autodiff)      â”‚  â”‚
    â”‚  â”‚  â”œâ”€ Update weights (AdamW)   â”‚  â”‚
    â”‚  â”‚  â””â”€ Save checkpoint          â”‚  â”‚
    â”‚  â”‚                              â”‚  â”‚
    â”‚  â””â”€â†’ TrainLogger.log()          â”‚  â”‚
    â”‚  â””â”€â†’ MetricsCSV.write()         â”‚  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–²
           â”‚ (repeat for N epochs)
           â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                 (checkpoint saved)
                       â”‚
                       â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       INFERENCE PHASE                â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚  Generate command              â”‚  â”‚
    â”‚  â”‚  â”œâ”€ Load model                 â”‚  â”‚
    â”‚  â”‚  â”œâ”€ Prompt â†’ encode            â”‚  â”‚
    â”‚  â”‚  â”œâ”€ Loop (max_tokens):         â”‚  â”‚
    â”‚  â”‚  â”‚  â”œâ”€ RWKV.forward_inference()â”‚  â”‚
    â”‚  â”‚  â”‚  â”œâ”€ Sample (temp, top-k)    â”‚  â”‚
    â”‚  â”‚  â”‚  â””â”€ Append token            â”‚  â”‚
    â”‚  â”‚  â””â”€ Decode â†’ text              â”‚  â”‚
    â”‚  â””â”€â”€â†’ Print output                â”‚  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âŒ CÃ“DIGO NÃƒO SENDO UTILIZADO

### CRÃTICO (Infraestrutura Pronta, Sem IntegraÃ§Ã£o):

#### **LoRA Fine-Tuning (140 linhas, 0 uso)**
```rust
// src/model/adapters.rs

pub struct LoRAAdapter<B: Backend> { ... }    // âŒ Nunca instanciado
pub struct DomainAdapterBank<B: Backend> { ... } // âŒ Nunca usado
pub struct DomainRegistry { ... }             // âŒ Nunca criado
pub struct DomainFineTuneConfig { ... }       // âŒ Nunca usado
```

**Impacto:** 
- Sem fine-tuning por domÃ­nio
- Todos 7 domÃ­nios predefinidos (Legal, Financial, Medical, etc) nÃ£o aproveitados
- ~32k-128k parÃ¢metros treinÃ¡veis por adapter = nÃ£o utilizados

**Como comeÃ§ar:**
```rust
// Nunca feito:
let adapter = LoRAAdapter::new(d_model, rank, scale, device);
let output = adapter.apply(x, base_output);
```

---

#### **Evaluator & EvalMetrics (94 + 264 = 358 linhas, pouco uso)**
```rust
// src/model/evaluator.rs

pub struct Evaluator { ... }     // âŒ Nunca instanciado no training loop
pub struct EvalMetrics { ... }   // âŒ Nunca retornado
```

**O que faz:**
- Calcula Perplexity
- Calcula Loss em validation set
- Calcula Accuracy

**Problema:** 
- `Trainer.validation_step()` existe mas nunca chamado
- Sem validaÃ§Ã£o proper durante training
- Sem early stopping

**EvidÃªncia:**
```rust
// src/main.rs - training loop
for batch in loader {
    trainer.train_step(input, target);
    // âŒ FALTA: trainer.validation_step()
}
```

---

#### **Gradient Checkpointing (93 linhas, 0 uso)**
```rust
// src/model/checkpoint.rs

pub struct CheckpointedActivation<B: Backend> { ... }  // âŒ Nunca usado
pub struct CheckpointConfig { ... }                    // âŒ Nunca usado
```

**PropÃ³sito:** Economizar RAM ao treinar modelos grandes
- Recomputa ativaÃ§Ãµes no backward ao invÃ©s de guardar
- Reduz RAM em ~50%, aumenta compute em ~20%

**Problema:** Nunca integrado ao forward pass

---

#### **GradScaler - Mixed Precision (57 linhas, 0 uso)**
```rust
// src/model/precision.rs

pub struct GradScaler { ... }  // âŒ Nunca instanciado
```

**PropÃ³sito:** FP16 + FP32 training (mais rÃ¡pido em CUDA)

**Problema:** Nunca usado em `Trainer`

---

### PARCIAL (CÃ³digo Pronto, Pouco Integrado):

#### **LRFinderResult - Learning Rate Finder (84 linhas, nunca acionado)**
```rust
// src/model/lr_finder.rs

pub struct LRFinderResult {
    pub learning_rates: Vec<f64>,
    pub losses: Vec<f32>,
    pub suggested_lr: f64,
}
```

**Status:** 
- âœ… Implementado
- âŒ Comando `FindLr` definido em CLI
- âŒ Nunca chamado em `main()`

**EvidÃªncia:**
```rust
// src/main.rs linha 411
Commands::FindLr { ... } => find_lr_cmd(...),
// âŒ find_lr_cmd() nunca adicionado ou sempre retorna Err
```

---

#### **RWKVState - Incremental Inference (criado mas vazio)**
```rust
pub struct RWKVState<B: Backend> {
    pub time_state: Vec<(Tensor, Tensor, Tensor)>,
    pub channel_state: Vec<Tensor>,
}
```

**O que Ã©:** Cache de estado para inferÃªncia token-by-token

**Status:** 
- âœ… Struct criado
- âš ï¸ Criado em alguns contextos
- âŒ Nunca alimentado no loop de geraÃ§Ã£o
- âŒ Generate faz forward completo a cada token (ineficiente)

**Impacto:** 
- Generate Ã© ~50x mais lento que poderia ser
- Para 100 tokens: 100 forward passes ao invÃ©s de 100 single-token forwards

---

#### **PTBRNormalizer - Parcialmente Integrado**
```rust
pub struct PTBRNormalizer { ... }
```

**Implementado:** âœ…
- NFD decomposition
- Lowercase
- Whitespace normalization

**IntegraÃ§Ã£o:** âš ï¸ Pouca
- Existe em `src/tokenizer/normalize.rs`
- Nunca explicitamente chamado no pipeline
- BPETokenizer normaliza internamente mas de forma limitada

---

## ğŸ“Š ESTATÃSTICA DE UTILIZAÃ‡ÃƒO

```
Total de Structs/Classes: 28
â”œâ”€ âœ… Ativamente Usados (Heavy): 10
â”‚  â”œâ”€ BPETokenizer
â”‚  â”œâ”€ MmapDataset
â”‚  â”œâ”€ DataLoader
â”‚  â”œâ”€ RWKV
â”‚  â”œâ”€ RWKVBlock
â”‚  â”œâ”€ TimeMixing
â”‚  â”œâ”€ ChannelMixing
â”‚  â”œâ”€ Trainer
â”‚  â”œâ”€ WikiCleaner
â”‚  â””â”€ TrainLogger
â”‚
â”œâ”€ âš ï¸ Parcialmente Usados: 4
â”‚  â”œâ”€ RWKVState (criado, nÃ£o alimentado)
â”‚  â”œâ”€ PTBRNormalizer (implementado, pouco integrado)
â”‚  â”œâ”€ LRFinderResult (pronto, nunca chamado)
â”‚  â””â”€ Evaluator (pronto, nunca integrado)
â”‚
â””â”€ âŒ Nunca Usados: 14
   â”œâ”€ LoRAAdapter
   â”œâ”€ DomainAdapterBank
   â”œâ”€ DomainRegistry
   â”œâ”€ DomainFineTuneConfig
   â”œâ”€ CheckpointedActivation
   â”œâ”€ CheckpointConfig
   â”œâ”€ GradScaler
   â”œâ”€ EvalMetrics
   â”œâ”€ BPETrainer (sÃ³ durante prep)
   â””â”€ ... (6 mais)

UtilizaÃ§Ã£o: ~35% do cÃ³digo
CÃ³digo Morto: ~65%
```

---

## ğŸ¯ RESUMO EXECUTIVO

### âœ… O Que Funciona (Core Pipeline):
```
Wiki/Corpus â†’ Clean â†’ Tokenize â†’ Binary Dataset â†’ 
DataLoader â†’ Trainer â†’ RWKV â†’ Forward/Backward â†’ 
Checkpoint â†’ Generate
```

**Status:** Funcional end-to-end
**Performance:** Otimizado com:
- LRU cache (tokenizer)
- Memory-mapped dataset
- Gradient accumulation
- Learning rate schedule

---

### âŒ O Que NÃ£o EstÃ¡ Sendo Usado:

1. **LoRA Fine-Tuning** - Infraestrutura pronta
   - 7 domÃ­nios definidos (Legal, Financial, Medical, Tech, Academic, News, General)
   - Nunca instanciado
   - Impacto: Sem fine-tuning eficiente

2. **Multi-Domain Adapters** - Infraestrutura pronta
   - DomainAdapterBank nunca criado
   - Impacto: Sem suporte a multi-task learning

3. **Proper Validation** - Evaluator pronto, nÃ£o integrado
   - Sem early stopping
   - Sem mÃ©tricas de validaÃ§Ã£o real

4. **Gradient Checkpointing** - Implementado, nÃ£o usado
   - Impacto: Alto uso de memÃ³ria em modelos grandes

5. **Mixed Precision** - GradScaler pronto, nÃ£o integrado
   - Impacto: ~2x mais lento que deveria ser

6. **RWKVState** - Para inferÃªncia eficiente
   - Criado, nunca alimentado
   - Impacto: Generate ~50x mais lento que poderia

---

## ğŸ”§ PRÃ“XIMOS PASSOS RECOMENDADOS

### Curto Prazo (CorreÃ§Ãµes):
1. âœ… Remover `#![allow(dead_code)]` e revisitar
2. âœ… Integrar Evaluator no Trainer
3. âœ… Ativar RWKVState na geraÃ§Ã£o

### MÃ©dio Prazo (Features):
1. Integrar LoRA fine-tuning
2. Ativar domain adapters
3. Implementar LR finder

### Longo Prazo (OtimizaÃ§Ãµes):
1. Gradient checkpointing
2. Mixed precision
3. Multi-GPU training

---

**Gerado:** 2026-01-12  
**VersÃ£o:** 1.0  
**ResponsÃ¡vel:** AnÃ¡lise AutomÃ¡tica
