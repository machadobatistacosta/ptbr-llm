# ğŸ“š PTBR-SLM - Arquitetura Completa

**Small Language Model em Rust focado em PortuguÃªs Brasileiro**

- **Data**: Janeiro 2026 (Atualizado)
- **VersÃ£o**: 0.1.0
- **Linguagem**: Rust 2021 Edition
- **Framework ML**: Burn (backend NdArray + Autodiff)
- **Status**: âœ… MÃºltiplas versÃµes em treinamento - v3 em desenvolvimento

---

## ğŸ“‹ Ãndice

1. [VisÃ£o Geral](#visÃ£o-geral)
2. [Arquitetura do Modelo](#arquitetura-do-modelo)
3. [MÃ³dulos do Projeto](#mÃ³dulos-do-projeto)
4. [Fluxo de Dados](#fluxo-de-dados)
5. [Estrutura de DiretÃ³rios](#estrutura-de-diretÃ³rios)
6. [DependÃªncias](#dependÃªncias)
7. [Comandos CLI](#comandos-cli)

---

## ğŸ¯ VisÃ£o Geral

O **PTBR-SLM** Ã© um modelo de linguagem pequeno baseado na arquitetura **RWKV** (RNN com Mixing Temporal), otimizado para:

- âœ… Funcionar em mÃ¡quinas com **8GB RAM**
- âœ… Suportar **mÃºltiplos tamanhos** de modelo (micro, mini, 85M)
- âœ… Processar texto em **PortuguÃªs Brasileiro**
- âœ… Treinamento iterativo com **checkpoints**
- âœ… Executar em **CPU** (backend NdArray)

### ConfiguraÃ§Ãµes DisponÃ­veis

| Config | Params | d_model | Layers | d_ffn | RAM |
|--------|--------|---------|--------|-------|-----|
| **micro** | 10M | 256 | 4 | 1024 | 2GB |
| **mini** | 20M | 384 | 6 | 1344 | 4GB |
| **85m** | 85M | 768 | 12 | 2688 | 8GB |

---

## ğŸ§  Arquitetura do Modelo

### RWKVConfig (ConfiguraÃ§Ã£o)

```rust
pub struct RWKVConfig {
    pub vocab_size: usize,       // 32.000 tokens
    pub d_model: usize,           // DimensÃ£o oculta
    pub n_layers: usize,          // NÃºmero de blocos
    pub d_ffn: usize,             // DimensÃ£o FFN
    pub max_seq_len: usize,       // Comprimento mÃ¡ximo
    pub dropout: f64,             // Taxa de dropout
    pub layer_norm_eps: f64,      // Epsilon do LayerNorm
}
```

**MÃ©todos Factory:**
- `RWKVConfig::ptbr_85m()` - 85M parÃ¢metros
- `RWKVConfig::ptbr_mini()` - 20M parÃ¢metros
- `RWKVConfig::ptbr_micro()` - 10M parÃ¢metros
- `num_parameters()` - CÃ¡lculo total de parÃ¢metros

---

### TrainingConfig (ConfiguraÃ§Ã£o de Treinamento)

```rust
pub struct TrainingConfig {
    pub learning_rate: f64,              // 3e-4
    pub batch_size: usize,               // 2
    pub gradient_accumulation_steps: usize, // 16
    pub warmup_steps: usize,             // 500
    pub max_steps: usize,                // 50.000
    pub weight_decay: f64,               // 0.01
    pub max_grad_norm: f64,              // 1.0
}
```

---

### RWKV (Modelo Principal)

```rust
pub struct RWKV<B: Backend> {
    embedding: Embedding<B>,
    blocks: Vec<RWKVBlock<B>>,
    ln_out: LayerNorm<B>,
    lm_head: Linear<B>,
}
```

**Pipeline:**
1. Embedding â†’ `[batch, seq_len, d_model]`
2. N blocos RWKV sequenciais
3. LayerNorm final
4. ProjeÃ§Ã£o linear â†’ logits `[batch, seq_len, vocab_size]`

---

### RWKVBlock (Bloco Fundamental)

Combina dois componentes principais:

```rust
pub struct RWKVBlock<B: Backend> {
    ln1: LayerNorm<B>,
    time_mixing: TimeMixing<B>,
    ln2: LayerNorm<B>,
    channel_mixing: ChannelMixing<B>,
    dropout: Dropout,
}
```

**Fluxo:**
```
x â†’ LayerNorm â†’ TimeMixing â†’ Dropout â†’ +
x â†’ LayerNorm â†’ ChannelMixing â†’ Dropout â†’ +
```

---

### TimeMixing (AnÃ¡logo a AtenÃ§Ã£o)

Implementa uma forma eficiente de atenÃ§Ã£o temporal sem operaÃ§Ãµes quadrÃ¡ticas:

```rust
pub struct TimeMixing<B: Backend> {
    key: Linear<B>,
    value: Linear<B>,
    receptance: Linear<B>,
    output: Linear<B>,
}
```

**OperaÃ§Ãµes:**
1. Shift temporal: `x_prev = concat([zeros, x[:-1]])`
2. Mix: `mixed = (x + x_prev) / 2`
3. ProjeÃ§Ãµes: `k = key(mixed)`, `v = value(mixed)`, `r = sigmoid(receptance(mixed))`
4. AtenÃ§Ã£o: `out = softmax(k) @ v`
5. Output: `output(r * out)`

**Vantagem:** Complexidade $O(n)$ em vez de $O(n^2)$

---

### ChannelMixing (FFN Eficiente)

Rede neural feedforward com ativaÃ§Ã£o **ReLU Quadrado**:

```rust
pub struct ChannelMixing<B: Backend> {
    key: Linear<B>,        // d_model â†’ d_ffn
    value: Linear<B>,      // d_ffn â†’ d_model
    receptance: Linear<B>, // d_model â†’ d_model
}
```

**OperaÃ§Ãµes:**
1. Shift temporal igual a TimeMixing
2. FFN: `k = key(mixed)`
3. AtivaÃ§Ã£o: `k_squared = ReLU(k)Â²`
4. Gating: `r = sigmoid(receptance(mixed))`
5. Output: `r * value(k_squared)`

**AtivaÃ§Ã£o ReLUÂ²:** $\max(0, x)^2$ - mais suave que ReLU padrÃ£o

---

### Trainer (Gerenciador de Treinamento)

```rust
pub struct Trainer<B: AutodiffBackend> {
    pub model: RWKV<B>,
    optimizer: OptimizerAdaptor<AdamW<B::InnerBackend>, RWKV<B>, B>,
    config: TrainingConfig,
    step: usize,
    device: B::Device,
}
```

**MÃ©todos Principais:**

- `new()` - Inicializa modelo, otimizador e config
- `train_step(input_ids, target_ids)` â†’ `f32`
  - Forward pass
  - CÃ¡lculo de cross-entropy loss
  - Backward pass (autograd)
  - AtualizaÃ§Ã£o de pesos com AdamW
  
- `cross_entropy_loss()` - Loss combinado de tokens
- `get_learning_rate()` - Warmup + linear decay
- `save_checkpoint(path)` - PersistÃªncia em `.mpk`
- `load_checkpoint(path)` - Carregamento de pesos

---

## ğŸ“¦ MÃ³dulos do Projeto

### `src/model/`

| Arquivo | Componente | Responsabilidade |
|---------|-----------|------------------|
| `config.rs` | RWKVConfig, TrainingConfig | ConfiguraÃ§Ãµes do modelo |
| `rwkv.rs` | RWKV, RWKVBlock, TimeMixing, ChannelMixing | Arquitetura do modelo |
| `trainer.rs` | Trainer | OtimizaÃ§Ã£o e treinamento |
| `adapters.rs` | (Auxiliar) | Adaptadores do Burn framework |
| `mod.rs` | Exports | Exporta estruturas pÃºblicas |

---

### `src/data/`

| Arquivo | Componente | Responsabilidade |
|---------|-----------|------------------|
| `wiki_parser.rs` | WikiStreamParser, WikiArticle | Parse streaming de dumps Wikipedia BZ2 |
| `cleaner.rs` | WikiCleaner | Limpeza de markup Wikipedia |
| `dataset.rs` | MmapDataset, DataLoader, TokenizedDatasetWriter | Dataset otimizado e I/O |
| `mod.rs` | Exports | Exporta estruturas pÃºblicas |

#### WikiStreamParser

Parse eficiente de dumps Wikipedia em formato BZ2:

```rust
pub struct WikiStreamParser {
    config: WikiParserConfig,
}
```

- **DescompactaÃ§Ã£o on-the-fly** com `bzip2`
- **Parser XML** com `quick_xml`
- **Iterator lazy** - processa sob demanda
- **Eficiente em memÃ³ria** - nÃ£o carrega tudo na RAM

#### WikiCleaner

Remove markup residual de Wikipedia:

```rust
pub struct WikiCleaner {
    min_sentence_len: usize,
}
```

**RemoÃ§Ãµes:**
- `<ref>` tags e referÃªncias
- `[[Ficheiro:...]]`, `[[Arquivo:...]]` - links de arquivo
- `[[Categoria:...]]` - categorias
- `{{...}}` - templates
- `<...>` - tags HTML
- `[[...|...]]` - links Wikipedia
- EspaÃ§os mÃºltiplos e quebras de linha excessivas
- Caracteres de controle

#### MmapDataset

Dataset otimizado com **memory-mapping**:

```rust
pub struct MmapDataset {
    data: Mmap,           // Arquivo mapeado em memÃ³ria
    indices: Vec<usize>,  // Ãndices de sequÃªncias
    seq_len: usize,       // Comprimento de sequÃªncia
}
```

**CaracterÃ­sticas:**
- LÃª arquivo tokenizado via `memmap2`
- Tokens armazenados como `u16` (2 bytes cada)
- NÃ£o carrega tudo na RAM
- Suporta shuffling com seed

**MÃ©todos:**
- `from_file(path, seq_len)` - Carrega dataset
- `get(idx)` â†’ `(input: Vec<u16>, target: Vec<u16>)`
- `shuffle(seed)` - Embaralha Ã­ndices
- `len()` - Quantidade de sequÃªncias

#### DataLoader

Iterator para batches do dataset:

```rust
pub struct DataLoader<'a> {
    dataset: &'a MmapDataset,
    batch_size: usize,
    current_idx: usize,
}
```

- Agrupa sequÃªncias em batches
- MantÃ©m estado de iteraÃ§Ã£o
- Iterator trait implementado

#### TokenizedDatasetWriter

Escritor eficiente de dados tokenizados:

```rust
pub struct TokenizedDatasetWriter {
    // Buffer para I/O otimizado
}
```

- Escreve tokens como `u16` em arquivo binÃ¡rio
- BufWriter para performance

---

### `src/tokenizer/`

| Arquivo | Componente | Responsabilidade |
|---------|-----------|------------------|
| `bpe.rs` | BPETokenizer, BPETrainer, BPEVocab | TokenizaÃ§Ã£o BPE |
| `normalize.rs` | PTBRNormalizer | NormalizaÃ§Ã£o PT-BR |
| `mod.rs` | Exports | Exporta estruturas pÃºblicas |

#### BPEVocab (VocabulÃ¡rio)

Estrutura serializÃ¡vel para JSON:

```rust
pub struct BPEVocab {
    pub id_to_token: Vec<Vec<u8>>,              // ID â†’ bytes
    pub merges: Vec<(u16, u16)>,                // HistÃ³rico de fusÃµes
    pub special_tokens: HashMap<String, u16>,   // Special tokens
}
```

#### BPETokenizer (Tokenizador)

```rust
pub struct BPETokenizer {
    id_to_token: Vec<Vec<u8>>,
    token_to_id: HashMap<Vec<u8>, u16>,
    merges: Vec<(u16, u16)>,
    special_tokens: HashMap<String, u16>,
    cache: HashMap<String, Vec<u16>>,  // Cache 10k entradas
}
```

**Tokens Especiais:**
- `[PAD]` (ID 0) - Padding
- `[UNK]` (ID 1) - Desconhecido
- `[BOS]` (ID 2) - InÃ­cio de sequÃªncia
- `[EOS]` (ID 3) - Fim de sequÃªncia
- `[SEP]` (ID 4) - Separador

**MÃ©todos:**
- `encode(text)` â†’ `Vec<u16>` (com cache)
- `decode(ids)` â†’ `String`
- `from_file(path)` - Carrega JSON
- `save(path)` - Salva JSON
- `vocab_size()` - Tamanho do vocabulÃ¡rio

**Caching:** 10.000 sequÃªncias tokenizadas em cache LRU

#### BPETrainer (Treinador)

```rust
pub struct BPETrainer {
    vocab_size: usize,
    min_frequency: usize,
}
```

- ConstrÃ³i vocabulÃ¡rio iterativamente
- **ParalelizaÃ§Ã£o com Rayon** para performance
- Suporta atÃ© 32.000 tokens

#### PTBRNormalizer (NormalizaÃ§Ã£o PT-BR)

```rust
pub struct PTBRNormalizer {
    // ConfiguraÃ§Ãµes
}
```

- Tratamento de acentos
- NormalizaÃ§Ã£o de pontuaÃ§Ã£o
- ConversÃ£o de casos

---

## ğŸ”„ Fluxo de Dados

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Wikipedia PT-BR BZ2 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ ProcessWiki (WikiStreamParser + WikiCleaner)
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Texto Limpo (TXT)     â”‚
â”‚ data/wiki_clean/       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ TrainTokenizer (BPETrainer)
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VocabulÃ¡rio BPE (JSON)  â”‚
â”‚ data/tokenizer_v2/       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ Tokenize (BPETokenizer)
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dataset Tokenizado (BIN)    â”‚
â”‚ data/tokenized_v2/           â”‚
â”‚ (tokens como u16 + mmap)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ DataLoader + Train (RWKV + Trainer)
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Modelo Treinado (MPK)   â”‚
â”‚ checkpoints_v2/          â”‚
â”‚ - checkpoint_XXXXX.bin   â”‚
â”‚ - model_final.mpk        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ Generate (InferÃªncia)
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Texto Gerado    â”‚
â”‚  PT-BR          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Estrutura de DiretÃ³rios

```
ptbr-slm/
â”œâ”€â”€ Cargo.toml                    # DependÃªncias do projeto
â”œâ”€â”€ Cargo.lock                    # Lock de versÃµes
â”œâ”€â”€ ARQUITETURA.md               # Este documento
â”œâ”€â”€ corpus.txt                   # Corpus de treinamento
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs                  # CLI e funÃ§Ãµes principais
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ config.rs           # RWKVConfig, TrainingConfig
â”‚   â”‚   â”œâ”€â”€ rwkv.rs             # RWKV, RWKVBlock, TimeMixing, ChannelMixing
â”‚   â”‚   â”œâ”€â”€ trainer.rs          # Trainer
â”‚   â”‚   â””â”€â”€ adapters.rs
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ wiki_parser.rs      # WikiStreamParser, WikiArticle
â”‚   â”‚   â”œâ”€â”€ cleaner.rs          # WikiCleaner
â”‚   â”‚   â””â”€â”€ dataset.rs          # MmapDataset, DataLoader
â”‚   â””â”€â”€ tokenizer/
â”‚       â”œâ”€â”€ mod.rs
â”‚       â”œâ”€â”€ bpe.rs              # BPETokenizer, BPETrainer, BPEVocab
â”‚       â””â”€â”€ normalize.rs        # PTBRNormalizer
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ model_85m.toml           # ConfiguraÃ§Ã£o modelo 85M
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Dados brutos
â”‚   â”œâ”€â”€ dumps/                   # Wikipedia BZ2 comprimidos
â”‚   â”œâ”€â”€ wiki_clean/              # Wikipedia limpa (TXT)
â”‚   â”œâ”€â”€ wiki_processed_v2/       # Wikipedia processada v2
â”‚   â”œâ”€â”€ planalto_clean/          # Corpus Planalto limpo
â”‚   â”œâ”€â”€ v15_clean/               # Corpus v15 limpo
â”‚   â”œâ”€â”€ wikibooks_clean/         # Wikibooks limpo
â”‚   â”œâ”€â”€ wikinews_clean/          # Wikinews limpo
â”‚   â”œâ”€â”€ wikisource_clean/        # Wikisource limpo
â”‚   â”œâ”€â”€ sovereign/               # Corpus adicional
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ corpus/              # Wikipedia parseada
â”‚   â”‚   â””â”€â”€ tokenized/           # Dataset antigo
â”‚   â”œâ”€â”€ tokenized_v2/            # Dataset tokenizado v2 (BIN)
â”‚   â”œâ”€â”€ tokenized_v3/            # Dataset tokenizado v3 (BIN)
â”‚   â”œâ”€â”€ tokenized_v12/           # Dataset tokenizado v12 (BIN)
â”‚   â”œâ”€â”€ tokenized_v15/           # Dataset tokenizado v15 (BIN)
â”‚   â”œâ”€â”€ tokenizer/
â”‚   â”‚   â””â”€â”€ tokenizer_v2/        # VocabulÃ¡rio v2 (JSON)
â”‚   â”œâ”€â”€ tokenizer_v2/            # Tokenizer v2 (JSON)
â”‚   â”œâ”€â”€ tokenizer_v3/            # Tokenizer v3 (JSON)
â”‚   â”œâ”€â”€ tokenizer_v12/           # Tokenizer v12 (JSON)
â”‚   â””â”€â”€ tokenizer_v15/           # Tokenizer v15 (JSON)
â”‚
â”œâ”€â”€ checkpoints/                 # Checkpoints v1 (legacy)
â”‚   â”œâ”€â”€ checkpoint_5000.mpk
â”‚   â”œâ”€â”€ checkpoint_10000.mpk
â”‚   â”œâ”€â”€ checkpoint_60000.mpk
â”‚   â””â”€â”€ model_final.mpk          # Modelo final v1
â”‚
â”œâ”€â”€ checkpoints_v2/              # Checkpoints v2 
â”‚   â”œâ”€â”€ checkpoint_2500.mpk
â”‚   â”œâ”€â”€ checkpoint_5000.mpk
â”‚   â”œâ”€â”€ checkpoint_30000.mpk
â”‚   â””â”€â”€ ... (atÃ© checkpoint_27500.mpk)
â”‚
â”œâ”€â”€ checkpoints_v3/              # Checkpoints v3 (atual)
â”‚   â”œâ”€â”€ checkpoint_2500.mpk
â”‚   â”œâ”€â”€ checkpoint_5000.mpk
â”‚   â”œâ”€â”€ checkpoint_45000.mpk     # â† Ãšltimo checkpoint
â”‚   â””â”€â”€ ... (atÃ© checkpoint_45000.mpk)
â”‚
â”œâ”€â”€ checkpoints_v12/             # Checkpoints v12 (archive)
â”œâ”€â”€ checkpoints_v12_micro/       # Checkpoints v12 micro (archive)
â”‚
â”œâ”€â”€ logs/                        # Logs de treinamento (vazio)
â”œâ”€â”€ scripts/                     # Scripts de processamento e limpeza
â”‚   â”œâ”€â”€ audit_sources_v15.py
â”‚   â”œâ”€â”€ build_corpus_v15_stream.py
â”‚   â”œâ”€â”€ clean_sources_v15.py
â”‚   â””â”€â”€ ... (diversos scripts de processamento)
â”‚
â””â”€â”€ target/
    â””â”€â”€ release/
        â””â”€â”€ ptbr-slm.exe         # ExecutÃ¡vel compilado
```

---

## ğŸ“š DependÃªncias

### Framework ML
- **burn** `0.14` - Framework deep learning
  - Features: `ndarray` (CPU), `autodiff`, `train`

### Processamento de Dados
- **quick-xml** `0.31` - Parser XML
- **bzip2** `0.4` - DescompactaÃ§Ã£o BZ2
- **memmap2** `0.9` - Memory-mapping de arquivos
- **serde** + **serde_json** - SerializaÃ§Ã£o JSON

### Texto
- **regex** `1.10` - ExpressÃµes regulares
- **once_cell** `1.19` - Lazy initialization
- **unicode-normalization** `0.1` - NormalizaÃ§Ã£o Unicode

### ConcorrÃªncia e Performance
- **crossbeam-channel** `0.5` - Channels thread-safe
- **rayon** `1.10` - Data parallelism

### Utilidades
- **rand** + **rand_chacha** `0.8/0.3` - RandomizaÃ§Ã£o
- **clap** `4.5` - CLI parser
- **tracing** + **tracing-subscriber** - Logging

### BinÃ¡rios
- **bincode** `2.0-rc3` - SerializaÃ§Ã£o binÃ¡ria

---

## âš™ï¸ Comandos CLI

### 1. Processar Wikipedia

```bash
./ptbr-slm.exe process-wiki \
  --input wiki-dump.xml.bz2 \
  --output data/wiki_clean
```

**Entrada:** Dump Wikipedia BZ2  
**SaÃ­da:** 
- `wiki_000.txt` (10k artigos cada)
- `wiki_001.txt`
- ...

---

### 2. Treinar Tokenizer BPE

```bash
./ptbr-slm.exe train-tokenizer \
  --corpus data/wiki_clean \
  --output data/tokenizer_v2 \
  --vocab-size 32000
```

**Entrada:** DiretÃ³rio com TXTs  
**SaÃ­da:** `tokenizer_v2/tokenizer.json`

---

### 3. Tokenizar Corpus

```bash
./ptbr-slm.exe tokenize \
  --input data/wiki_clean \
  --output data/tokenized_v2 \
  --tokenizer data/tokenizer_v2/tokenizer.json
```

**Entrada:** 
- TXTs (texto limpo)
- JSON (vocabulÃ¡rio BPE)

**SaÃ­da:** Arquivo binÃ¡rio com tokens u16

---

### 4. Treinar Modelo

```bash
./ptbr-slm.exe train \
  --data data/tokenized_v2 \
  --tokenizer data/tokenizer_v2/tokenizer.json \
  --output checkpoints_v2 \
  --model-size micro
```

**ParÃ¢metros:**
- `--model-size`: `micro` (10M), `mini` (20M), ou `85m` (85M)

**SaÃ­da:**
- `checkpoint_2500.bin`
- `checkpoint_5000.bin`
- ... (a cada 2500 steps)

---

### 5. Continuar Treinamento

```bash
./ptbr-slm.exe resume \
  --checkpoint checkpoints_v2/checkpoint_27500.bin \
  --data data/tokenized_v2 \
  --output checkpoints_v2 \
  --additional-steps 50000 \
  --model-size micro
```

**Carrega checkpoint e treina mais 50k steps**

---

### 6. Testar Modelo

```bash
./ptbr-slm.exe test-model \
  --model checkpoints_v2/checkpoint_27500.bin \
  --tokenizer data/tokenizer_v2/tokenizer.json
```

**Mostra top-5 prediÃ§Ãµes para tokens aleatÃ³rios**

---

### 7. Gerar Texto

```bash
./ptbr-slm.exe generate \
  --model checkpoints_v2/checkpoint_27500.bin \
  --tokenizer data/tokenizer_v2/tokenizer.json \
  --prompt "O Brasil Ã© um paÃ­s que" \
  --max-tokens 100
```

**SaÃ­da:** ContinuaÃ§Ã£o de texto em PortuguÃªs Brasileiro

**Exemplo:**
```
Prompt: "O Brasil Ã© um paÃ­s que"
Generated: "O Brasil Ã© um paÃ­s que possui uma rica histÃ³ria e cultura. Com mais de 200 milhÃµes de habitantes, Ã© o maior paÃ­s da AmÃ©rica do Sul..."
```

---

### 8. Limpar Corpus

```bash
./ptbr-slm.exe clean-corpus \
  --input data/raw \
  --output data/wiki_clean \
  --verbose true
```

**Remove markup residual de Wikipedia**

---

## ğŸ”§ Build e ExecuÃ§Ã£o

### Compilar Release

```bash
cargo build --release
```

**OtimizaÃ§Ãµes:**
- LTO (Link Time Optimization)
- Single codegen unit
- Optimization level 3

### Executar

```bash
.\target\release\ptbr-slm.exe <COMANDO>
```

---

## ğŸ“Š EstatÃ­sticas do Projeto

| MÃ©trica | Valor |
|---------|-------|
| **Linhas de CÃ³digo** | ~2000+ |
| **Arquivos Rust** | 10+ |
| **MÃ³dulos Principais** | 3 (model, data, tokenizer) |
| **Estruturas Principais** | 12+ |
| **Comandos CLI** | 8 |
| **Tamanho Vocab** | 32.000 tokens |
| **Modelos Suportados** | 3 (micro, mini, 85m) |
| **Max Seq Length** | 256-2048 |
| **VersÃµes Treinadas** | 5 (v1, v2, v3, v12, v15) |
| **Total de Checkpoints** | 100+ |
| **Corpora Processados** | 6+ |
| **Scripts de Processamento** | 20+

---

## ğŸ“ Conceitos-Chave

### Arquitetura RWKV

RWKV (RNN with Gated Linear Recurrence) oferece:
- âœ… Complexidade linear $O(n)$ vs transformers $O(n^2)$
- âœ… Long-range dependencies sem atenÃ§Ã£o quadrÃ¡tica
- âœ… EficiÃªncia em memÃ³ria
- âœ… Treinamento paralelo

### Time Mixing vs Channel Mixing

- **Time Mixing**: Processa ao longo da sequÃªncia (temporal)
- **Channel Mixing**: Processa ao longo das dimensÃµes (feedforward)

Combinados formam um bloco eficiente equivalente a Transformer.

### BPE Tokenization

Byte Pair Encoding:
1. ComeÃ§a com bytes individuais
2. Iterativamente funde pares mais frequentes
3. AtÃ© atingir tamanho de vocabulÃ¡rio desejado

**Vantagens:**
- Suporta qualquer caractere
- VocabulÃ¡rio compacto
- Subword units

---

## ğŸ“ˆ Status de Treinamento

**Status Atual:** Janeiro 2026 - MÃºltiplas versÃµes em desenvolvimento

### Checkpoints DisponÃ­veis

| VersÃ£o | Ãšltima | Steps | Status | Dados |
|--------|--------|-------|--------|-------|
| **v1** | checkpoint_60000 | 60k | âœ… Finalizado | Wikipedia v1 |
| **v2** | checkpoint_27500 | 27.5k | â¸ï¸ Pausado | Wikipedia v2 |
| **v3** | checkpoint_45000 | 45k | ğŸ”„ Ativo | Wiki + Planalto + Wikisource |
| **v12** | checkpoint_30000 | 30k | ğŸ“¦ Archive | Corpus diversificado |
| **v12_micro** | checkpoint_5000 | 5k | ğŸ“¦ Archive | Modelo micro experimental |

### Dados e Tokenizers Treinados

| VersÃ£o | Tokenizer | Dataset | Corpora |
|--------|-----------|---------|---------|
| **v2** | âœ… 32k vocab | tokenized_v2 | Wikipedia pt-br |
| **v3** | âœ… 32k vocab | tokenized_v3 | Wiki + Planalto + Wikisource |
| **v12** | âœ… 32k vocab | tokenized_v12 | Multi-corpora |
| **v15** | âœ… 32k vocab | tokenized_v15 | Ultra-clean |

### Corpora DisponÃ­veis

- âœ… Wikipedia (wiki_clean, wiki_processed_v2)
- âœ… Planalto (legislaÃ§Ã£o e atos do governo)
- âœ… Wikisource (obras literÃ¡rias)
- âœ… Wikibooks, Wikinews (conteÃºdo adicional)
- âœ… Corpus Soberano (v15 ultra-limpo)

### PrÃ³ximos Passos

- [ ] Finalizar v3 training (45k â†’ 100k steps)
- [ ] Avaliar qualidade v3 vs v2
- [ ] Fine-tuning em domÃ­nios especÃ­ficos
- [ ] Benchmark de performance
- [ ] OtimizaÃ§Ãµes de quantizaÃ§Ã£o

---

## ğŸ” Notas TÃ©cnicas

### Backend Burn

Usando **NdArray** (CPU) em vez de GPU:
- Compatibilidade universal
- Sem dependÃªncias de drivers
- Ideal para desenvolvimento
- Pode ser trocado para GPU posteriormente

### Memory-Mapping

Dataset usa `memmap2` para:
- NÃ£o carregar arquivo inteiro na RAM
- Acesso rÃ¡pido a qualquer parte
- Escalabilidade

### Autodiff

Burn + Autodiff providencia:
- Automatic differentiation
- Backward pass automÃ¡tico
- Gradientes computados eficientemente

---

## ğŸ“ Autores e LicenÃ§a

- **Autor:** Caike Machado Batista Costa
- **VersÃ£o:** 0.1.0
- **Edition:** Rust 2021
- **Data AtualizaÃ§Ã£o:** Janeiro 2026

---

## ğŸš€ Timeline do Projeto

### Fase 1: Foundation (âœ… ConcluÃ­da)
- [x] Arquitetura RWKV implementada em Rust
- [x] Framework Burn integrado (NdArray backend)
- [x] CLI bÃ¡sico com 8 comandos
- [x] Dataset v1 com Wikipedia

### Fase 2: Escalabilidade (âœ… ConcluÃ­da)
- [x] Memory-mapping para datasets grandes
- [x] MÃºltiplas versÃµes de tokenizers
- [x] Processamento paralelo com Rayon
- [x] Checkpoints em v2 e v3

### Fase 3: ExpansÃ£o de Dados (âœ… ConcluÃ­da)
- [x] IntegraÃ§Ã£o Planalto (legislaÃ§Ã£o)
- [x] Wikisource (obras literÃ¡rias)
- [x] Wikibooks e Wikinews
- [x] Ultra-clean corpus v15

### Fase 4: OtimizaÃ§Ã£o (ğŸ”„ Em Progresso)
- [ ] MÃ©tricas de avaliaÃ§Ã£o
- [ ] Fine-tuning especializado
- [ ] QuantizaÃ§Ã£o e compressÃ£o
- [ ] Backend GPU (Wgpu)

### Fase 5: ProduÃ§Ã£o (â³ Planejado)
- [ ] API REST
- [ ] ContainerizaÃ§Ã£o (Docker)
- [ ] Deployment em cloud
- [ ] Benchmarks pÃºblicos
