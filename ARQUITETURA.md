# ğŸ“š PTBR-SLM - Arquitetura Completa

**Small Language Model em Rust focado em PortuguÃªs Brasileiro**

- **Data**: Janeiro 2026 (Atualizado)
- **VersÃ£o**: 0.1.0
- **Linguagem**: Rust 2021 Edition
- **Framework ML**: Burn (backend NdArray + Autodiff)
- **Status**: âœ… MÃºltiplas versÃµes em treinamento - v3 em desenvolvimento

---

## ğŸ“‹ Ãndice

1. [VisÃ£o Geral](#visÃ£o-geral)
2. [Arquitetura do Modelo](#arquitetura-do-modelo)
3. [MÃ³dulos do Projeto](#mÃ³dulos-do-projeto)
4. [Fluxo de Dados](#fluxo-de-dados)
5. [Estrutura de DiretÃ³rios](#estrutura-de-diretÃ³rios)
6. [DependÃªncias](#dependÃªncias)
7. [Comandos CLI](#comandos-cli)

---

## ğŸ¯ VisÃ£o Geral

O **PTBR-SLM** Ã© um modelo de linguagem pequeno baseado na arquitetura **RWKV** (RNN com Mixing Temporal), otimizado para:

- âœ… Funcionar em mÃ¡quinas com **8GB RAM**
- âœ… Suportar **mÃºltiplos tamanhos** de modelo (micro, mini, 85M)
- âœ… Processar texto em **PortuguÃªs Brasileiro**
- âœ… Treinamento iterativo com **checkpoints**
- âœ… Executar em **CPU** (backend NdArray)

### ConfiguraÃ§Ãµes DisponÃ­veis

| Config | Params | d_model | Layers | d_ffn | RAM |
|--------|--------|---------|--------|-------|-----|
| **micro** | 10M | 256 | 4 | 1024 | 2GB |
| **mini** | 20M | 384 | 6 | 1344 | 4GB |
| **85m** | 85M | 768 | 12 | 2688 | 8GB |

---

## ğŸ§  Arquitetura do Modelo

### RWKVConfig (ConfiguraÃ§Ã£o)

```rust
#[derive(Config, Debug)]
pub struct RWKVConfig {
    #[config(default = "32000")]
    pub vocab_size: usize,         // Tamanho do vocabulÃ¡rio
    
    #[config(default = "768")]
    pub d_model: usize,             // DimensÃ£o oculta (embedding + outputs)
    
    #[config(default = "12")]
    pub n_layers: usize,            // NÃºmero de blocos RWKV sequenciais
    
    #[config(default = "2688")]
    pub d_ffn: usize,               // DimensÃ£o da rede feedforward
    
    #[config(default = "2048")]
    pub max_seq_len: usize,         // Comprimento mÃ¡ximo de sequÃªncia
    
    #[config(default = "0.1")]
    pub dropout: f64,               // Taxa de dropout para regularizaÃ§Ã£o
    
    #[config(default = "1e-5")]
    pub layer_norm_eps: f64,        // Epsilon de estabilidade do LayerNorm
}
```

**MÃ©todos Factory (PrÃ©-configuraÃ§Ãµes):**

| MÃ©todo | Params | d_model | Layers | d_ffn | max_seq | RAM | Uso |
|--------|--------|---------|--------|-------|---------|-----|-----|
| `ptbr_85m()` | 85M | 768 | 12 | 2688 | 2048 | 8GB | ProduÃ§Ã£o, benchmark |
| `ptbr_mini()` | 20M | 384 | 6 | 1344 | 512 | 4GB | Desenvolvimento, testes rÃ¡pidos |
| `ptbr_micro()` | 10M | 256 | 4 | 1024 | 256 | 2GB | Testes, prototipagem |

**CÃ¡lculo de ParÃ¢metros:**

```rust
pub fn num_parameters(&self) -> usize {
    let embedding = vocab_size * d_model;                    // Embedding layer
    let time_mixing = 5 * d_model * d_model * n_layers;      // TimeMixing layers
    let channel_mixing = 2 * d_model * d_ffn * n_layers;     // ChannelMixing layers
    let layer_norms = 4 * d_model * n_layers;                // LayerNorm weights
    embedding + time_mixing + channel_mixing + layer_norms
}
```

**Exemplo de cÃ¡lculo para 85M:**
- Embedding: 32k Ã— 768 = 24.6M
- TimeMixing: 5 Ã— 768 Ã— 768 Ã— 12 = 35.8M
- ChannelMixing: 2 Ã— 768 Ã— 2688 Ã— 12 = 50.3M
- LayerNorm: 4 Ã— 768 Ã— 12 = 36.9K
- **Total â‰ˆ 85M parÃ¢metros**

---

### TrainingConfig (ConfiguraÃ§Ã£o de Treinamento)

```rust
#[derive(Config, Debug)]
pub struct TrainingConfig {
    #[config(default = "3e-4")]
    pub learning_rate: f64,              // Taxa de aprendizado inicial
    
    #[config(default = "2")]
    pub batch_size: usize,               // Tamanho do batch
    
    #[config(default = "16")]
    pub gradient_accumulation_steps: usize, // AcumulaÃ§Ã£o de gradientes
    
    #[config(default = "500")]
    pub warmup_steps: usize,             // Steps de warmup linear
    
    #[config(default = "50000")]
    pub max_steps: usize,                // Total de steps de treinamento
    
    #[config(default = "0.01")]
    pub weight_decay: f64,               // RegularizaÃ§Ã£o L2 (AdamW)
    
    #[config(default = "1.0")]
    pub gradient_clip: f64,              // Valor mÃ¡ximo para gradient clipping
    
    #[config(default = "2500")]
    pub save_every: usize,               // Salvar checkpoint a cada N steps
    
    #[config(default = "500")]
    pub eval_every: usize,               // Avaliar a cada N steps
}

impl Default for TrainingConfig {
    fn default() -> Self {
        Self {
            learning_rate: 3e-4,
            batch_size: 2,
            gradient_accumulation_steps: 16,
            warmup_steps: 500,
            max_steps: 50_000,
            weight_decay: 0.01,
            gradient_clip: 1.0,
            save_every: 2500,
            eval_every: 500,
        }
    }
}
```

**Pipeline de Treinamento:**

1. **InicializaÃ§Ã£o** (inÃ­cio de cada epoch/run)
   - Cria modelo RWKV com config
   - Inicializa otimizador AdamW com weight_decay
   - Carrega dataset tokenizado em memÃ³ria mapeada

2. **Loop de Treinamento**
   - Para cada batch: inputs `[batch_size, seq_len]` â†’ targets `[batch_size, seq_len]`
   - Forward pass â†’ logits `[batch_size, seq_len, vocab_size]`
   - Calcula cross-entropy loss
   - Backward pass com autograd
   - Update de pesos com learning rate dinÃ¢mico

3. **Checkpointing**
   - A cada `save_every` steps: salva modelo em `.mpk` com CompactRecorder
   - Nomeia como `checkpoint_{step}.bin`
   - Permite retomar treinamento via `resume` command

4. **Learning Rate Schedule**
   - **Fase Warmup** (steps 0 a `warmup_steps`):  
     $lr = learning\_rate \times \frac{step}{warmup\_steps}$
   - **Fase Decay** (steps `warmup_steps` a `max_steps`):  
     $lr = learning\_rate \times (1 - 0.9 \times progress)$ com mÃ­nimo de 0.1  
     onde $progress = \frac{step - warmup\_steps}{max\_steps - warmup\_steps}$

---

### RWKV (Modelo Principal)

```rust
pub struct RWKV<B: Backend> {
    embedding: Embedding<B>,
    blocks: Vec<RWKVBlock<B>>,
    ln_out: LayerNorm<B>,
    lm_head: Linear<B>,
}
```

**Pipeline:**
1. Embedding â†’ `[batch, seq_len, d_model]`
2. N blocos RWKV sequenciais
3. LayerNorm final
4. ProjeÃ§Ã£o linear â†’ logits `[batch, seq_len, vocab_size]`

---

### RWKVBlock (Bloco Fundamental)

Combina dois componentes principais:

```rust
pub struct RWKVBlock<B: Backend> {
    ln1: LayerNorm<B>,
    time_mixing: TimeMixing<B>,
    ln2: LayerNorm<B>,
    channel_mixing: ChannelMixing<B>,
    dropout: Dropout,
}
```

**Fluxo:**
```
x â†’ LayerNorm â†’ TimeMixing â†’ Dropout â†’ +
x â†’ LayerNorm â†’ ChannelMixing â†’ Dropout â†’ +
```

---

### TimeMixing (AnÃ¡logo a AtenÃ§Ã£o)

Implementa uma forma eficiente de atenÃ§Ã£o temporal sem operaÃ§Ãµes quadrÃ¡ticas:

```rust
pub struct TimeMixing<B: Backend> {
    key: Linear<B>,
    value: Linear<B>,
    receptance: Linear<B>,
    output: Linear<B>,
}
```

**OperaÃ§Ãµes:**
1. Shift temporal: `x_prev = concat([zeros, x[:-1]])`
2. Mix: `mixed = (x + x_prev) / 2`
3. ProjeÃ§Ãµes: `k = key(mixed)`, `v = value(mixed)`, `r = sigmoid(receptance(mixed))`
4. AtenÃ§Ã£o: `out = softmax(k) @ v`
5. Output: `output(r * out)`

**Vantagem:** Complexidade $O(n)$ em vez de $O(n^2)$

---

### ChannelMixing (FFN Eficiente)

Rede neural feedforward com ativaÃ§Ã£o **ReLU Quadrado**:

```rust
pub struct ChannelMixing<B: Backend> {
    key: Linear<B>,        // d_model â†’ d_ffn
    value: Linear<B>,      // d_ffn â†’ d_model
    receptance: Linear<B>, // d_model â†’ d_model
}
```

**OperaÃ§Ãµes:**
1. Shift temporal igual a TimeMixing
2. FFN: `k = key(mixed)`
3. AtivaÃ§Ã£o: `k_squared = ReLU(k)Â²`
4. Gating: `r = sigmoid(receptance(mixed))`
5. Output: `r * value(k_squared)`

**AtivaÃ§Ã£o ReLUÂ²:** $\max(0, x)^2$ - mais suave que ReLU padrÃ£o

---

### Trainer (Gerenciador de Treinamento)

```rust
pub struct Trainer<B: AutodiffBackend> {
    pub model: RWKV<B>,
    optimizer: OptimizerAdaptor<AdamW<B::InnerBackend>, RWKV<B>, B>,
    config: TrainingConfig,
    step: usize,
    device: B::Device,
}
```

**Pipeline de Treinamento:**

1. **InicializaÃ§Ã£o** (`new`)
   - Cria modelo RWKV com config
   - Inicializa otimizador AdamW uma Ãºnica vez
   - Armazena config de treinamento

2. **Forward Pass** (`train_step`)
   - Embedding â†’ SequÃªncia de blocos RWKV â†’ LayerNorm â†’ LM Head
   - Entrada: `[batch_size, seq_len, vocab_size]`
   - SaÃ­da: logits

3. **Loss Calculation** (`cross_entropy_loss`)
   - Flattens logits e targets
   - Log-softmax nos logits
   - Negative log-likelihood (NLL) dos targets
   - Mean reduction

4. **Backward Pass & Weight Update** 
   - Automatic differentiation (autograd)
   - Compute gradients com `.backward()`
   - Update pesos com `optimizer.step()`
   - Learning rate scheduling (warmup + decay)

**MÃ©todos Principais:**

- `new(model_config, train_config, device)` â†’ Trainer
  - Cria modelo RWKV
  - Inicializa AdamW com weight decay
  - Retorna instÃ¢ncia do trainer

- `train_step(input_ids, target_ids)` â†’ `f32`
  - Forward pass com modelo
  - Calcula cross-entropy loss
  - Backward pass (autograd) 
  - Atualiza pesos com learning rate dinamicamente
  - Retorna valor escalar do loss
  
- `cross_entropy_loss(logits, targets)` â†’ `Tensor<B, 1>`
  - Flatten logits e targets
  - Log-softmax + NLL
  - Mean reduction
  - Negativo para loss

- `get_learning_rate()` â†’ `f64`
  - Warmup linear: $lr_{initial} \times \frac{step}{warmup\_steps}$ se $step < warmup\_steps$
  - Decay linear: $lr_{initial} \times (1 - 0.9 \times progress)$ com mÃ­nimo 0.1
  - Onde $progress = \frac{step - warmup\_steps}{max\_steps - warmup\_steps}$

- `save_checkpoint(path)` â†’ `std::io::Result<()>`
  - Serializa modelo com CompactRecorder
  - Salva em formato `.mpk`
  - Remove extensÃ£o do path automaticamente

- `load_checkpoint(&mut self, path)` â†’ `std::io::Result<()>`
  - Desserializa modelo do arquivo `.mpk`
  - Restaura pesos para device
  - Printa confirmaÃ§Ã£o do carregamento

- `step()` â†’ `usize` - Retorna contador de steps
- `config()` â†’ `&TrainingConfig` - Retorna config imutÃ¡vel

**Tipos de Backend:**

- `B: AutodiffBackend` - Backend com suporte a autograd
  - `B::InnerBackend` - Backend subjacente (ex: NdArray)
  - `B::Device` - Device (CPU/GPU)

**Otimizador:**

```rust
AdamW {
    learning_rate: f64,      // Taxa de aprendizado
    beta_1: f64,             // 0.9 (momentum)
    beta_2: f64,             // 0.999 (adaptive rate)
    epsilon: f64,            // Estabilidade numÃ©rica
    weight_decay: f64,       // RegularizaÃ§Ã£o L2
}
```

Com adaptador para integraÃ§Ã£o com Burn's gradient system.

---

## ğŸ“¦ MÃ³dulos do Projeto

### `src/model/`

| Arquivo | Componente | Responsabilidade |
|---------|-----------|------------------|
| `config.rs` | RWKVConfig, TrainingConfig | ConfiguraÃ§Ãµes do modelo (85M/mini/micro) |
| `rwkv.rs` | RWKV, RWKVBlock, TimeMixing, ChannelMixing | Arquitetura RWKV completa |
| `trainer.rs` | Trainer | Loop de treinamento, loss, backward, checkpoints |
| `adapters.rs` | OptimizerAdaptor | Adaptador de otimizador para Burn |
| `mod.rs` | Exports | Re-exporta structs pÃºblicas |

**Linhas de CÃ³digo por Arquivo:**
- `config.rs`: ~124 linhas
- `rwkv.rs`: ~400+ linhas (modelo + blocos)
- `trainer.rs`: ~131 linhas (treinamento)
- `adapters.rs`: ~50+ linhas (adaptadores)
- `mod.rs`: ~10 linhas

---

### `src/data/`

| Arquivo | Componente | Responsabilidade |
|---------|-----------|------------------|
| `wiki_parser.rs` | WikiStreamParser, WikiArticle, WikiParserConfig | Parse streaming BZ2, Iterator lazy |
| `cleaner.rs` | WikiCleaner | Remove markup, normaliza qualidade |
| `dataset.rs` | MmapDataset, DataLoader, TokenizedDatasetWriter | Memory-mapping, batching, I/O |
| `mod.rs` | Exports | Re-exporta estruturas pÃºblicas |

**Linhas de CÃ³digo por Arquivo:**
- `wiki_parser.rs`: ~200+ linhas (parser + iterador)
- `cleaner.rs`: ~150+ linhas (regex + limpeza)
- `dataset.rs`: ~155 linhas (mmap + dataloader)
- `mod.rs`: ~10 linhas

#### WikiStreamParser

Parse eficiente de dumps Wikipedia em formato BZ2:

```rust
pub struct WikiStreamParser {
    config: WikiParserConfig,
}
```

- **DescompactaÃ§Ã£o on-the-fly** com `bzip2`
- **Parser XML** com `quick_xml`
- **Iterator lazy** - processa sob demanda
- **Eficiente em memÃ³ria** - nÃ£o carrega tudo na RAM

#### WikiCleaner

Remove markup residual de Wikipedia:

```rust
pub struct WikiCleaner {
    min_sentence_len: usize,
}
```

**RemoÃ§Ãµes:**
- `<ref>` tags e referÃªncias
- `[[Ficheiro:...]]`, `[[Arquivo:...]]` - links de arquivo
- `[[Categoria:...]]` - categorias
- `{{...}}` - templates
- `<...>` - tags HTML
- `[[...|...]]` - links Wikipedia
- EspaÃ§os mÃºltiplos e quebras de linha excessivas
- Caracteres de controle

#### MmapDataset (Dataset Otimizado com Memory-Mapping)

```rust
pub struct MmapDataset {
    data: Mmap,           // Arquivo mapeado em memÃ³ria (read-only)
    indices: Vec<usize>,  // Ãndices de inÃ­cio de cada sequÃªncia
    seq_len: usize,       // Comprimento esperado de sequÃªncia
}
```

**CaracterÃ­sticas:**
- LÃª arquivo tokenizado via `memmap2::Mmap` (nÃ£o carrega tudo na RAM)
- Tokens armazenados como `u16` little-endian (2 bytes cada)
- Suporta shuffling determinÃ­stico com seed (ChaCha8)
- Ãndices prÃ©-calculados para acesso O(1)

**MÃ©todos:**
- `from_file(path, seq_len)` - Carrega dataset com inferÃªncia de tamanho
  - Calcula: `num_tokens = file_size / 2`
  - Calcula: `num_sequences = (num_tokens - seq_len) / seq_len`
  
- `get(idx)` â†’ `Option<(Vec<u16>, Vec<u16>)>` - Retorna (input, target)
  - input: tokens 0..seq_len
  - target: tokens 1..(seq_len+1) - shift de 1 para prediÃ§Ã£o de prÃ³ximo token
  
- `shuffle(seed)` - Embaralha Ã­ndices com ChaCha8Rng (determinÃ­stico)
  
- `len()` - NÃºmero total de sequÃªncias disponÃ­veis
  
- `is_empty()` - Verifica se dataset estÃ¡ vazio

**Exemplo de uso:**
```rust
let dataset = MmapDataset::from_file("data/tokenized_v3/train.bin", 256)?;
println!("Sequences: {}", dataset.len());  // ex: 1,000,000

dataset.shuffle(42);  // DeterminÃ­stico

if let Some((input, target)) = dataset.get(0) {
    println!("Input: {:?}", input);    // Vec<u16> com 256 tokens
    println!("Target: {:?}", target);  // Vec<u16> com 256 tokens
}
```

#### DataLoader (Iterator com Batching)

```rust
pub struct DataLoader<'a> {
    dataset: &'a MmapDataset,
    batch_size: usize,
    current_idx: usize,
}

impl<'a> DataLoader<'a> {
    pub fn new(dataset: &'a MmapDataset, batch_size: usize) -> Self { ... }
    pub fn reset(&mut self) { ... }
}

impl<'a> Iterator for DataLoader<'a> {
    type Item = (Vec<Vec<u16>>, Vec<Vec<u16>>);  // (inputs, targets)
    fn next(&mut self) -> Option<Self::Item> { ... }
}
```

**Funcionalidade:**
- Agrupa sequÃªncias do dataset em batches
- MantÃ©m estado interno de iteraÃ§Ã£o
- Implementa padrÃ£o Iterator de Rust
- Cada chamada a `next()` retorna `(batch_inputs, batch_targets)`
  - `batch_inputs`: Vec com `batch_size` sequÃªncias de input
  - `batch_targets`: Vec com `batch_size` sequÃªncias de target

**Exemplo:**
```rust
let dataset = MmapDataset::from_file("train.bin", 256)?;
let loader = DataLoader::new(&dataset, 32);  // batch_size=32

for (batch_inputs, batch_targets) in loader {
    // batch_inputs: Vec<Vec<u16>> com 32 sequÃªncias de 256 tokens
    // batch_targets: Vec<Vec<u16>> com 32 sequÃªncias de 256 tokens
    let loss = trainer.train_step(inputs_tensor, targets_tensor);
}
```

#### TokenizedDatasetWriter (SerializaÃ§Ã£o de Tokens)

```rust
pub struct TokenizedDatasetWriter {
    writer: BufWriter<File>,
    tokens_written: usize,
}

impl TokenizedDatasetWriter {
    pub fn new(path: &Path) -> std::io::Result<Self> { ... }
    pub fn write_tokens(&mut self, tokens: &[u16]) -> std::io::Result<()> { ... }
    pub fn finish(mut self) -> std::io::Result<usize> { ... }
}
```

**Funcionalidade:**
- Escreve tokens em arquivo binÃ¡rio com buffer otimizado (1MB)
- Cada token `u16` Ã© serializado em little-endian (2 bytes)
- `finish()` faz flush e retorna total de tokens escritos

**Exemplo:**
```rust
let mut writer = TokenizedDatasetWriter::new("data/tokenized_v3/train.bin")?;

let tokens = tokenizer.encode("O Brasil Ã© um paÃ­s..."); // Vec<u16>
writer.write_tokens(&tokens)?;

let total = writer.finish()?;  // Flush e retorna count
println!("Total tokens: {}", total);
```

**Fluxo Completo:**
```
Texto em PT-BR
    â†“
BPETokenizer::encode()  â†’ Vec<u16> (tokens)
    â†“
TokenizedDatasetWriter::write_tokens()  â†’ Arquivo binÃ¡rio
    â†“
MmapDataset::from_file()  â†’ Carrega para treinamento
```

---

### `src/tokenizer/`

| Arquivo | Componente | Responsabilidade |
|---------|-----------|------------------|
| `bpe.rs` | BPETokenizer, BPETrainer, BPEVocab | TokenizaÃ§Ã£o BPE, treinamento vocab |
| `normalize.rs` | PTBRNormalizer | NormalizaÃ§Ã£o PT-BR (acentos, espaÃ§o) |
| `mod.rs` | Exports | Re-exporta estruturas pÃºblicas |

**Linhas de CÃ³digo por Arquivo:**
- `bpe.rs`: ~500+ linhas (tokenizador + trainer + vocab)
- `normalize.rs`: ~100+ linhas (normalizador)
- `mod.rs`: ~10 linhas

#### BPEVocab (Estrutura de VocabulÃ¡rio)

```rust
#[derive(Serialize, Deserialize, Clone)]
pub struct BPEVocab {
    pub id_to_token: Vec<Vec<u8>>,      // Mapeamento ID â†’ bytes do token
    pub merges: Vec<(u16, u16)>,        // HistÃ³rico de fusÃµes em ordem
    pub special_tokens: HashMap<String, u16>,  // Tokens especiais
}

impl BPEVocab {
    pub fn new() -> Self { ... }
    pub fn build_token_to_id(&self) -> HashMap<Vec<u8>, u16> { ... }
}
```

**Armazenamento:**
- Serializada em JSON (`tokenizer.json`)
- ContÃ©m tudo necessÃ¡rio para reconstruir BPETokenizer
- Tamanho tÃ­pico: 200-500KB para vocab de 32k

**Exemplo de JSON:**
```json
{
  "id_to_token": [
    [112, 97, 100],      // "pad"
    [117, 110, 107],     // "unk"
    ...
  ],
  "merges": [
    [0, 1],   // Funde tokens 0 e 1
    [2, 3],   // Funde tokens 2 e 3
    ...
  ],
  "special_tokens": {
    "[PAD]": 0,
    "[UNK]": 1,
    ...
  }
}
```

#### BPETokenizer (Tokenizador Principal)

```rust
pub struct BPETokenizer {
    id_to_token: Vec<Vec<u8>>,
    token_to_id: HashMap<Vec<u8>, u16>,
    merges: Vec<(u16, u16)>,
    special_tokens: HashMap<String, u16>,
    cache: HashMap<String, Vec<u16>>,  // LRU cache com atÃ© 10k entradas
}
```

**Tokens Especiais (IDs 0-4):**
- `[PAD]` (ID 0) - Padding para sequÃªncias curtas
- `[UNK]` (ID 1) - Token desconhecido (fallback)
- `[BOS]` (ID 2) - Beginning of sequence (inÃ­cio)
- `[EOS]` (ID 3) - End of sequence (fim)
- `[SEP]` (ID 4) - Separador (para pares de textos)

**MÃ©todos Principais:**

- `encode(text: &str)` â†’ `Vec<u16>` (com cache LRU)
  - Normaliza texto com PTBRNormalizer
  - Converte em bytes individuais como tokens iniciais
  - Aplica merges do BPE iterativamente
  - Verifica cache: hit evita recomputaÃ§Ã£o
  - Retorna IDs dos tokens finais

- `decode(ids: &[u16])` â†’ `String`
  - Converte IDs de volta para bytes
  - Junta bytes em string UTF-8
  - Trata tokens especiais corretamente

- `from_file(path: &str)` â†’ `Result<Self>`
  - Carrega `tokenizer.json` salvo com serde
  - ReconstrÃ³i token_to_id a partir de id_to_token
  - Retorna tokenizador pronto

- `save(path: &str)` â†’ `Result<()>`
  - Serializa em JSON com serde_json
  - Formato: { id_to_token, merges, special_tokens }

- `vocab_size()` â†’ `usize`
  - Retorna: `id_to_token.len()`

- `eos_id()` â†’ `u16`
  - Retorna ID do token [EOS]

**Cache LRU:**
- Limita a 10.000 entradas para nÃ£o crescer infinitamente
- Melhora performance em textos repetitivos
- Implementado com HashMap (nÃ£o Ã© LRU puro, mas aproximado)

#### BPETrainer (Treinador de VocabulÃ¡rio)

```rust
pub struct BPETrainer {
    vocab_size: usize,
    min_frequency: usize,
}

impl BPETrainer {
    pub fn new(vocab_size: usize, min_frequency: usize) -> Self { ... }
    pub fn train(self, texts: Box<dyn Iterator<Item = String>>) -> BPEVocab { ... }
}
```

**Algoritmo BPE (Byte Pair Encoding):**

1. **InicializaÃ§Ã£o:** ComeÃ§a com todos os bytes Ãºnicos (0-255) + special tokens
   - Inicial: 259 tokens (256 bytes + 5 specials - 2 overlap)

2. **IteraÃ§Ã£o Merging:**
   - Conta frequÃªncia de pares adjacentes em todo o corpus
   - Funde o par mais frequente em um novo token
   - Incrementa vocab de 1
   - Repete atÃ© atingir `vocab_size`

3. **Resultado:**
   - 32.000 tokens = 256 bytes + 31.744 merges
   - Cada merge cria um novo token com IDs 256+

**ParalelizaÃ§Ã£o com Rayon:**
- Processa corpus em paralelo
- Cada thread mantÃ©m contagem local de frequÃªncias
- Sincroniza para encontrar melhor merge
- Muito mais rÃ¡pido para corpus grande

**Exemplo:**
```rust
let trainer = BPETrainer::new(32_000, 2);  // Vocab 32k, min freq 2

let texts = Box::new(
    std::fs::read_dir("data/wiki_clean")?
        .filter_map(|e| e.ok())
        .flat_map(|e| std::fs::read_to_string(e.path()).ok().into_iter())
        .flat_map(|c| c.lines().map(String::from).collect::<Vec<_>>())
);

let vocab = trainer.train(texts);
let tokenizer = BPETokenizer::from_vocab(vocab);
tokenizer.save("data/tokenizer_v3/tokenizer.json")?;
```

**Tempo de Treinamento:**
- 1 GB de texto: ~5-10 minutos (com Rayon)
- 10 GB de texto: ~30-60 minutos
- Depende de: CPU cores, velocidade de I/O, tamanho final do vocab

#### PTBRNormalizer (NormalizaÃ§Ã£o Portuguesa)

```rust
pub struct PTBRNormalizer {
    // ConfiguraÃ§Ãµes internas
}

impl PTBRNormalizer {
    pub fn new() -> Self { ... }
    pub fn normalize(text: &str) -> String { ... }
}
```

**NormalizaÃ§Ãµes Aplicadas:**

1. **AcentuaÃ§Ã£o:**
   - Unicode NFD decomposition (separa base de diacrÃ­ticos)
   - Opcionalmente remove acentos (configurÃ¡vel)

2. **PontuaÃ§Ã£o:**
   - Padroniza tipos de aspas: " â†’ " (Unicode)
   - Padroniza travessÃµes: - â†’ â€”
   - EspaÃ§os ao redor de pontuaÃ§Ã£o

3. **EspaÃ§amento:**
   - Remove espaÃ§os mÃºltiplos â†’ espaÃ§o Ãºnico
   - Normaliza quebras de linha: \r\n, \r â†’ \n
   - Remove espaÃ§os no inÃ­cio/fim de linhas

4. **Case:** 
   - MantÃ©m case original (nÃ£o forÃ§a lowercase)
   - Preserva siglas: "PT" vs "Portugal"

5. **Caracteres de Controle:**
   - Remove: \x00-\x08, \x0b-\x0c, \x0e-\x1f
   - MantÃ©m: \n (\x0a), \t (\x09)

**Exemplo:**
```rust
let text = "  O  Brasil   Ã©  um  paÃ­s...  ";
let normalized = PTBRNormalizer::normalize(text);
// Resultado: "O Brasil Ã© um paÃ­s..."

let text_acentos = "SÃ£o Paulo - maÃ§Ã£";
// Com NFD: "Sa~o Paulo - mac~a" (base + diacrÃ­ticos separados)
```

**Quando Ã© Aplicado:**
- Durante `encode()` do BPETokenizer (antes de tokenizar)
- Garante consistÃªncia: "BRASIL" e "Brasil" sÃ£o normalizados

---

## ğŸ”„ Fluxo de Dados

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Wikipedia PT-BR BZ2 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ ProcessWiki (WikiStreamParser + WikiCleaner)
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Texto Limpo (TXT)     â”‚
â”‚ data/wiki_clean/       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ TrainTokenizer (BPETrainer)
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VocabulÃ¡rio BPE (JSON)  â”‚
â”‚ data/tokenizer_v2/       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ Tokenize (BPETokenizer)
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dataset Tokenizado (BIN)    â”‚
â”‚ data/tokenized_v2/           â”‚
â”‚ (tokens como u16 + mmap)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ DataLoader + Train (RWKV + Trainer)
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Modelo Treinado (MPK)   â”‚
â”‚ checkpoints_v2/          â”‚
â”‚ - checkpoint_XXXXX.bin   â”‚
â”‚ - model_final.mpk        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ Generate (InferÃªncia)
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Texto Gerado    â”‚
â”‚  PT-BR          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Estrutura de DiretÃ³rios

```
ptbr-slm/
â”œâ”€â”€ Cargo.toml                    # DependÃªncias do projeto
â”œâ”€â”€ Cargo.lock                    # Lock de versÃµes
â”œâ”€â”€ ARQUITETURA.md               # Este documento
â”œâ”€â”€ corpus.txt                   # Corpus de treinamento
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs                  # CLI e funÃ§Ãµes principais
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ config.rs           # RWKVConfig, TrainingConfig
â”‚   â”‚   â”œâ”€â”€ rwkv.rs             # RWKV, RWKVBlock, TimeMixing, ChannelMixing
â”‚   â”‚   â”œâ”€â”€ trainer.rs          # Trainer
â”‚   â”‚   â””â”€â”€ adapters.rs
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ wiki_parser.rs      # WikiStreamParser, WikiArticle
â”‚   â”‚   â”œâ”€â”€ cleaner.rs          # WikiCleaner
â”‚   â”‚   â””â”€â”€ dataset.rs          # MmapDataset, DataLoader
â”‚   â””â”€â”€ tokenizer/
â”‚       â”œâ”€â”€ mod.rs
â”‚       â”œâ”€â”€ bpe.rs              # BPETokenizer, BPETrainer, BPEVocab
â”‚       â””â”€â”€ normalize.rs        # PTBRNormalizer
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ model_85m.toml           # ConfiguraÃ§Ã£o modelo 85M
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Dados brutos
â”‚   â”œâ”€â”€ dumps/                   # Wikipedia BZ2 comprimidos
â”‚   â”œâ”€â”€ wiki_clean/              # Wikipedia limpa (TXT)
â”‚   â”œâ”€â”€ wiki_processed_v2/       # Wikipedia processada v2
â”‚   â”œâ”€â”€ planalto_clean/          # Corpus Planalto limpo
â”‚   â”œâ”€â”€ v15_clean/               # Corpus v15 limpo
â”‚   â”œâ”€â”€ wikibooks_clean/         # Wikibooks limpo
â”‚   â”œâ”€â”€ wikinews_clean/          # Wikinews limpo
â”‚   â”œâ”€â”€ wikisource_clean/        # Wikisource limpo
â”‚   â”œâ”€â”€ sovereign/               # Corpus adicional
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ corpus/              # Wikipedia parseada
â”‚   â”‚   â””â”€â”€ tokenized/           # Dataset antigo
â”‚   â”œâ”€â”€ tokenized_v2/            # Dataset tokenizado v2 (BIN)
â”‚   â”œâ”€â”€ tokenized_v3/            # Dataset tokenizado v3 (BIN)
â”‚   â”œâ”€â”€ tokenized_v12/           # Dataset tokenizado v12 (BIN)
â”‚   â”œâ”€â”€ tokenized_v15/           # Dataset tokenizado v15 (BIN)
â”‚   â”œâ”€â”€ tokenizer/
â”‚   â”‚   â””â”€â”€ tokenizer_v2/        # VocabulÃ¡rio v2 (JSON)
â”‚   â”œâ”€â”€ tokenizer_v2/            # Tokenizer v2 (JSON)
â”‚   â”œâ”€â”€ tokenizer_v3/            # Tokenizer v3 (JSON)
â”‚   â”œâ”€â”€ tokenizer_v12/           # Tokenizer v12 (JSON)
â”‚   â””â”€â”€ tokenizer_v15/           # Tokenizer v15 (JSON)
â”‚
â”œâ”€â”€ checkpoints/                 # Checkpoints v1 (legacy)
â”‚   â”œâ”€â”€ checkpoint_5000.mpk
â”‚   â”œâ”€â”€ checkpoint_10000.mpk
â”‚   â”œâ”€â”€ checkpoint_60000.mpk
â”‚   â””â”€â”€ model_final.mpk          # Modelo final v1
â”‚
â”œâ”€â”€ checkpoints_v2/              # Checkpoints v2 
â”‚   â”œâ”€â”€ checkpoint_2500.mpk
â”‚   â”œâ”€â”€ checkpoint_5000.mpk
â”‚   â”œâ”€â”€ checkpoint_30000.mpk
â”‚   â””â”€â”€ ... (atÃ© checkpoint_27500.mpk)
â”‚
â”œâ”€â”€ checkpoints_v3/              # Checkpoints v3 (atual)
â”‚   â”œâ”€â”€ checkpoint_2500.mpk
â”‚   â”œâ”€â”€ checkpoint_5000.mpk
â”‚   â”œâ”€â”€ checkpoint_45000.mpk     # â† Ãšltimo checkpoint
â”‚   â””â”€â”€ ... (atÃ© checkpoint_45000.mpk)
â”‚
â”œâ”€â”€ checkpoints_v12/             # Checkpoints v12 (archive)
â”œâ”€â”€ checkpoints_v12_micro/       # Checkpoints v12 micro (archive)
â”‚
â”œâ”€â”€ logs/                        # Logs de treinamento (vazio)
â”œâ”€â”€ scripts/                     # Scripts Python de processamento
â”‚   â”œâ”€â”€ audit_sources_v15.py              # Auditoria de fontes v15
â”‚   â”œâ”€â”€ build_corpus_v15_stream.py        # Build streaming v15
â”‚   â”œâ”€â”€ build_corpus_v15_v2_clean.py      # Build alternativo v15
â”‚   â”œâ”€â”€ clean_sources_v15.py              # Limpeza de markup v15
â”‚   â”œâ”€â”€ clean_sovereign.py                # Processa corpus soberano
â”‚   â”œâ”€â”€ extract_chroma.py                 # Extrai caracterÃ­sticas
â”‚   â”œâ”€â”€ filter_v11_brasil_only.py         # Filtro Brasil-only (legacy)
â”‚   â”œâ”€â”€ filter_v12_ultra_clean.py         # Filtro ultra-clean v12 (legacy)
â”‚   â”œâ”€â”€ filter_v13_keep_more.py           # Filtro mais permissivo v13
â”‚   â”œâ”€â”€ filter_v14_quick_clean.py         # Filtro rÃ¡pido v14
â”‚   â”œâ”€â”€ fix_newlines_to_lf.py             # Normaliza quebras de linha
â”‚   â”œâ”€â”€ planalto_codigo_civil_v3.py       # Processa legislaÃ§Ã£o v3
â”‚   â”œâ”€â”€ planalto_resolve_codigo_civil.py  # Processa CC v1 (legacy)
â”‚   â”œâ”€â”€ planalto_resolve_codigo_civil_v2.py # Processa CC v2
â”‚   â”œâ”€â”€ scrape_planalto_fix_frames.py     # Scraper Planalto com frames
â”‚   â”œâ”€â”€ scrape_planalto_seeds.py          # Scraper Planalto v1 (legacy)
â”‚   â”œâ”€â”€ scrape_planalto_seeds_v2.py       # Scraper Planalto v2
â”‚   â””â”€â”€ unify_corpus.py                   # Unifica mÃºltiplas fontes
â”‚
â””â”€â”€ target/
    â””â”€â”€ release/
        â””â”€â”€ ptbr-slm.exe         # ExecutÃ¡vel compilado
```

---

## ğŸ“š DependÃªncias

### Framework ML
- **burn** `0.14` - Framework deep learning
  - Features: `ndarray` (CPU), `autodiff`, `train`
  - GPU Feature: `wgpu` (opcional, feature `gpu`)

### Processamento de Dados
- **quick-xml** `0.31` - Parser XML
- **bzip2** `0.4` - DescompactaÃ§Ã£o BZ2
- **memmap2** `0.9` - Memory-mapping de arquivos
- **serde** + **serde_json** - SerializaÃ§Ã£o JSON

### Texto
- **regex** `1.10` - ExpressÃµes regulares
- **once_cell** `1.19` - Lazy initialization
- **unicode-normalization** `0.1` - NormalizaÃ§Ã£o Unicode

### ConcorrÃªncia e Performance
- **crossbeam-channel** `0.5` - Channels thread-safe
- **rayon** `1.10` - Data parallelism

### Utilidades
- **rand** + **rand_chacha** `0.8/0.3` - RandomizaÃ§Ã£o
- **clap** `4.5` - CLI parser
- **tracing** + **tracing-subscriber** - Logging

### SerializaÃ§Ã£o
- **bincode** `2.0-rc3` - SerializaÃ§Ã£o binÃ¡ria (checkpoint save/load)

---

## âš™ï¸ Comandos CLI

### 1. Processar Wikipedia

```bash
./ptbr-slm.exe process-wiki \
  --input wiki-dump.xml.bz2 \
  --output data/wiki_clean
```

**Entrada:** Dump Wikipedia BZ2  
**SaÃ­da:** 
- `wiki_000.txt` (10k artigos cada)
- `wiki_001.txt`
- ...

---

### 2. Treinar Tokenizer BPE

```bash
./ptbr-slm.exe train-tokenizer \
  --corpus data/wiki_clean \
  --output data/tokenizer_v2 \
  --vocab-size 32000
```

**Entrada:** DiretÃ³rio com TXTs  
**SaÃ­da:** `tokenizer_v2/tokenizer.json`

---

### 3. Tokenizar Corpus

```bash
./ptbr-slm.exe tokenize \
  --input data/wiki_clean \
  --output data/tokenized_v2 \
  --tokenizer data/tokenizer_v2/tokenizer.json
```

**Entrada:** 
- TXTs (texto limpo)
- JSON (vocabulÃ¡rio BPE)

**SaÃ­da:** Arquivo binÃ¡rio com tokens u16

---

### 4. Treinar Modelo

```bash
./ptbr-slm.exe train \
  --data data/tokenized_v3 \
  --tokenizer data/tokenizer_v3/tokenizer.json \
  --output checkpoints_v3 \
  --model-size micro \
  --max-steps 100000 \
  --save-every 2500 \
  --batch-size 8 \
  --learning-rate 3e-4 \
  --warmup-steps 1000 \
  --seq-len 256
```

**Entrada:**
- `--data`: DiretÃ³rio com arquivo `train.bin` (tokens u16)
- `--tokenizer`: Caminho para `tokenizer.json`

**ParÃ¢metros de Modelo:**
- `--model-size`: `micro` (10M), `mini` (20M), ou `85m` (85M)

**ParÃ¢metros de Treinamento:**
- `--max-steps`: Total de steps (default: 50000)
- `--save-every`: Salvar checkpoint a cada N steps (default: 2500)
- `--batch-size`: Tamanho do batch (default: 2)
- `--learning-rate`: Taxa de aprendizado (default: 3e-4)
- `--warmup-steps`: Steps de warmup linear (default: 500)
- `--seq-len`: Comprimento de sequÃªncia (default: 256)
- `--grad-accum`: AcumulaÃ§Ã£o de gradientes (default: 16)

**SaÃ­da:**
- `checkpoints_v3/checkpoint_2500.bin`
- `checkpoints_v3/checkpoint_5000.bin`
- ... (a cada `save_every` steps)
- `checkpoints_v3/model_step_100000.bin` (modelo final)

**Output no Terminal:**
```
Modelo criado com 10.5M parÃ¢metros
  Iniciando do step 0...

Step    100 | Loss: 4.5234 | 2.45 steps/s | ETA: 11h23m
Step    200 | Loss: 3.8456 | 2.48 steps/s | ETA: 11h15m
  Checkpoint salvo: "checkpoints_v3/checkpoint_2500.bin"
...
```

---

### 5. Continuar Treinamento (Resume)

```bash
./ptbr-slm.exe resume \
  --checkpoint checkpoints_v3/checkpoint_45000.bin \
  --data data/tokenized_v3 \
  --output checkpoints_v3 \
  --additional-steps 55000 \
  --model-size micro \
  --save-every 2500 \
  --batch-size 8
```

**Entrada:**
- `--checkpoint`: Caminho do checkpoint a continuar (ex: `checkpoint_45000.bin`)
- `--data`: Dataset de treinamento
- `--additional-steps`: Quantos steps adicionais treinar (nÃ£o total!)

**Funcionalidade:**
- Carrega pesos salvos do checkpoint
- Recupera step interno do nome do arquivo
- Continua treinamento a partir desse ponto
- Salva novos checkpoints com step correto

**Exemplo PrÃ¡tico:**
- Se checkpoint Ã© `checkpoint_45000.bin` e `--additional-steps 55000`:
- Treinamento vai de step 45000 atÃ© 100000
- Vai salvar em: 47500, 50000, 52500, ..., 100000

**Vantagens:**
- RecuperaÃ§Ã£o de falhas (crash, interrupÃ§Ã£o)
- Treinamento em fases (ajustar LR, batch size, etc)
- EficiÃªncia: reutiliza pesos jÃ¡ aprendidos

---

### 6. Testar Modelo (Inference)

```bash
./ptbr-slm.exe test-model \
  --model checkpoints_v3/checkpoint_45000.bin \
  --tokenizer data/tokenizer_v3/tokenizer.json \
  --model-size micro
```

**Entrada:**
- `--model`: Caminho do checkpoint a testar
- `--tokenizer`: VocabulÃ¡rio tokenizador
- `--model-size`: Tamanho do modelo (deve ser o mesmo do treinamento)

**Funcionalidade:**
- Carrega modelo e tokenizador
- Seleciona tokens aleatÃ³rios do vocabulÃ¡rio
- Mostra top-5 prediÃ§Ãµes com probabilidades
- Permite avaliar qualidade de forma simples

**Exemplo de Output:**
```
Testando modelo...

Token: "Brasil"
  1. " Ã©" (prob: 0.234)
  2. " um" (prob: 0.189)
  3. " paÃ­s" (prob: 0.145)
  4. " de" (prob: 0.098)
  5. " estado" (prob: 0.076)

Token: "linguagem"
  1. " de" (prob: 0.312)
  2. " portuguesa" (prob: 0.256)
  3. " Ã©" (prob: 0.148)
  4. " programa" (prob: 0.089)
  5. " artificial" (prob: 0.065)
```

**UtilitÃ¡rio Para:**
- Debug inicial: modelo carrega/roda?
- Qualidade informal: prediÃ§Ãµes fazem sentido?
- Comparar versÃµes: v3 vs v2

---

### 7. Gerar Texto (Generation)

```bash
./ptbr-slm.exe generate \
  --model checkpoints_v3/checkpoint_45000.bin \
  --tokenizer data/tokenizer_v3/tokenizer.json \
  --prompt "O Brasil Ã© um paÃ­s" \
  --max-tokens 100 \
  --model-size micro \
  --temperature 0.8
```

**Entrada:**
- `--prompt`: Texto inicial para continuar
- `--max-tokens`: NÃºmero mÃ¡ximo de tokens a gerar (default: 100)
- `--temperature`: Controla aleatoriedade (0.1=determinÃ­stico, 2.0=muito aleatÃ³rio)
  - 0.5-0.8: Bom balanÃ§o
  - 0.3: Conservador, prediÃ§Ãµes Ã³bvias
  - 1.2: Mais criativo, mas pode sair do contexto

**Algoritmo:**
1. Tokeniza prompt â†’ `Vec<u16>`
2. Para cada step atÃ© `max-tokens`:
   - Forward pass: Ãºltimos tokens â†’ logits
   - Aplica temperature: divide logits por temperature
   - Computa softmax: logits â†’ probabilidades
   - Amostra next token com WeightedIndex
   - Se token == [EOS]: para
   - SenÃ£o: adiciona token e continua

3. Decodifica tokens â†’ string

**Exemplo de Output:**
```
Prompt: O Brasil Ã© um paÃ­s
Temperatura: 0.8
Gerado: O Brasil Ã© um paÃ­s que possui uma rica histÃ³ria cultural e natural. Com mais de 200 milhÃµes de habitantes e uma vasta extensÃ£o territorial, o Brasil Ã© conhecido mundialmente pela sua biodiversidade Ãºnica, pelas suas florestas tropicais e pela sua populaÃ§Ã£o diversa. A economia brasileira Ã© uma das maiores do mundo, com setores importantes como a agricultura...
```

**Notas:**
- Qualidade melhora com mais treinamento (steps)
- Temperature afeta coerÃªncia vs diversidade
- Modelo pode repetir ou divergir (limitaÃ§Ãµes do modelo pequeno)

---

### 8. Limpar Corpus

```bash
./ptbr-slm.exe clean-corpus \
  --input data/raw \
  --output data/wiki_clean \
  --verbose true
```

**Entrada:**
- `--input`: DiretÃ³rio com arquivos TXT (brutos de Wikipedia)

**Funcionalidade:**
- Remove markup residual de Wikipedia
- Filtra blocos de texto de baixa qualidade
- Detecta e remove "garbage" (markup deixado)
- Salva versÃ£o limpa

**RemoÃ§Ãµes Aplicadas:**

1. **Tags XML/HTML:**
   - `<ref>...</ref>` - ReferÃªncias
   - `<!--...-->` - ComentÃ¡rios HTML
   - Tags genÃ©ricas: `<...>`

2. **Markup Wikipedia:**
   - `[[Ficheiro:...]]`, `[[Arquivo:...]]` - Links de arquivo
   - `[[Categoria:...]]` - Categorias
   - `[[...|...]]` - Links internos com pipe
   - `{{...}}` - Templates

3. **Caracteres Especiais:**
   - `{{`, `}}` - Brackets duplos
   - `|-`, `|` - Delimitadores de tabela
   - `align=`, `width=`, `colspan=` - Atributos HTML

4. **FormataÃ§Ã£o:**
   - EspaÃ§os mÃºltiplos â†’ espaÃ§o Ãºnico
   - Quebras de linha excessivas â†’ mÃ¡x 2 quebras
   - Caracteres de controle removidos

**Filtro de Qualidade:**
- Remove blocos com < 100 caracteres
- Remove blocos sem linhas com > 50 caracteres
- Remove blocos que contÃªm garbage apÃ³s limpeza

**Output:**
```
===================================================
  Limpando corpus
===================================================

  Encontrados 15 arquivos

  wiki_001.txt: 2540KB -> 1820KB
  wiki_002.txt: 2310KB -> 1650KB
  ...

===================================================
  Arquivos salvos: 15
  Tamanho: 45.2MB -> 32.1MB (28.9% removido)
===================================================
```

**Tempo:**
- 1 GB: ~5 minutos
- 10 GB: ~30-40 minutos

---

## ğŸ”§ Build e ExecuÃ§Ã£o

### Compilar Release

```bash
cargo build --release
```

**OtimizaÃ§Ãµes Aplicadas** (em `Cargo.toml`):
```toml
[profile.release]
lto = true              # Link Time Optimization
codegen-units = 1      # Single code gen unit (mais otimizado, mais lento)
opt-level = 3          # MÃ¡xima otimizaÃ§Ã£o
```

**Tempo de compilaÃ§Ã£o:**
- Primeira vez: 3-5 minutos (download deps)
- Subsequentes: ~1-2 minutos

**Resultado:**
```
./target/release/ptbr-slm.exe  (~50-80 MB)
```

### Executar Comandos

```bash
# Windows
.\target\release\ptbr-slm.exe <COMANDO>

# Linux/Mac
./target/release/ptbr-slm.exe <COMANDO>
```

### Build de Desenvolvimento (Debug)

```bash
cargo build  # Sem --release
cargo run -- <COMANDO>
```

**Nota:** Build Debug Ã© 10-100x mais lento. Use apenas para debugging!

---

## ï¿½ Workflow Completo: Do Zero ao Modelo Treinado

### CenÃ¡rio: Treinar Novo Modelo v4 do Zero

**Premissas:**
- Wikipedia PT-BR em `data/dumps/` (em formato BZ2)
- 8GB RAM disponÃ­vel
- ~30 horas de tempo de treino

### Passo 1: Processar Wikipedia (1-2h)

```bash
.\target\release\ptbr-slm.exe process-wiki \
  --input data/dumps/pt-latest-pages-articles.xml.bz2 \
  --output data/wiki_raw_v4
```

**Resultado:** Arquivos TXT em `data/wiki_raw_v4/`
- Estrutura: `wiki_000.txt`, `wiki_001.txt`, ...
- Cada arquivo: ~10.000 artigos
- Total: ~500MB-1GB

### Passo 2: Limpar Corpus (1h)

```bash
.\target\release\ptbr-slm.exe clean-corpus \
  --input data/wiki_raw_v4 \
  --output data/wiki_clean_v4 \
  --verbose true
```

**Resultado:** Corpus limpo em `data/wiki_clean_v4/`
- RemoÃ§Ã£o de markup, limpeza de qualidade
- ReduÃ§Ã£o: ~30-40% do tamanho original

### Passo 3: Treinar Tokenizer (10-20m)

```bash
.\target\release\ptbr-slm.exe train-tokenizer \
  --corpus data/wiki_clean_v4 \
  --output data/tokenizer_v4 \
  --vocab-size 32000
```

**Resultado:** `data/tokenizer_v4/tokenizer.json`
- VocabulÃ¡rio BPE com 32.000 tokens
- ContÃ©m: base + merges + special tokens

### Passo 4: Tokenizar Dataset (30-60m)

```bash
.\target\release\ptbr-slm.exe tokenize \
  --input data/wiki_clean_v4 \
  --output data/tokenized_v4 \
  --tokenizer data/tokenizer_v4/tokenizer.json
```

**Resultado:** `data/tokenized_v4/train.bin`
- Arquivo binÃ¡rio com tokens u16
- Tamanho: ~200-400MB (dependendo do corpus)
- Pronto para treinamento

### Passo 5: Treinar Modelo (20-30h)

```bash
.\target\release\ptbr-slm.exe train \
  --data data/tokenized_v4 \
  --tokenizer data/tokenizer_v4/tokenizer.json \
  --output checkpoints_v4 \
  --model-size micro \
  --max-steps 100000 \
  --save-every 2500 \
  --batch-size 4 \
  --warmup-steps 1000 \
  --seq-len 256
```

**Durante o Treinamento:**
- Checkpoints salvos: 40x (a cada 2.500 steps)
- Terminal mostra: loss, steps/sec, ETA
- Prova de conceito em ~1 hora (10k steps)

**Resultado:** `checkpoints_v4/`
- `checkpoint_2500.bin`, `checkpoint_5000.bin`, ...
- `model_step_100000.bin` (modelo final)

### Passo 6: Testar Modelo

```bash
.\target\release\ptbr-slm.exe test-model \
  --model checkpoints_v4/checkpoint_100000.bin \
  --tokenizer data/tokenizer_v4/tokenizer.json \
  --model-size micro
```

### Passo 7: Gerar Texto

```bash
.\target\release\ptbr-slm.exe generate \
  --model checkpoints_v4/checkpoint_100000.bin \
  --tokenizer data/tokenizer_v4/tokenizer.json \
  --prompt "O Brasil Ã©" \
  --max-tokens 150 \
  --temperature 0.8
```

### Timeline Resumido

| Etapa | Tempo | Output |
|-------|-------|--------|
| Process | 1-2h | wiki_raw_v4/ |
| Clean | 0.5-1h | wiki_clean_v4/ |
| Tokenize Training | 0.25-0.5h | tokenizer_v4/ |
| Tokenize Data | 0.5-1h | tokenized_v4/ |
| Train | 20-30h | checkpoints_v4/ |
| **TOTAL** | **~24-34h** | **Modelo pronto!** |

### Alternativa: Continuar Treinamento Existente

Se jÃ¡ existe `checkpoint_50000.bin` e quer continuar:

```bash
.\target\release\ptbr-slm.exe resume \
  --checkpoint checkpoints_v3/checkpoint_50000.bin \
  --data data/tokenized_v3 \
  --output checkpoints_v3 \
  --additional-steps 50000 \
  --model-size micro
```

- Vai treinar atÃ© step 100.000 (50k + 50k)
- Muito mais rÃ¡pido: reutiliza pesos jÃ¡ aprendidos
- Tempo: ~10-15h (em vez de 25h)

| MÃ©trica | Valor |
|---------|-------|
| **Linhas de CÃ³digo Rust** | ~2500+ |
| **Arquivos Rust** | 11 |
| **MÃ³dulos Principais** | 3 (model, data, tokenizer) |
| **Estruturas Principais** | 14+ |
| **Comandos CLI** | 8 |
| **Tamanho Vocab** | 32.000 tokens |
| **Modelos Suportados** | 3 (micro, mini, 85m) |
| **Max Seq Length** | 256-2048 |
| **VersÃµes Treinadas** | 5 (v1, v2, v3, v12, v15) |
| **Total de Checkpoints** | 100+ |
| **Corpora Processados** | 6+ (Wikipedia, Planalto, Wikisource, Wikibooks, Wikinews, Sovereign) |
| **Scripts de Processamento** | 18 |
| **DependÃªncias Diretas** | 16 |

---

## ğŸ“ Conceitos-Chave

### Arquitetura RWKV

RWKV (RNN with Gated Linear Recurrence) oferece:
- âœ… Complexidade linear $O(n)$ vs transformers $O(n^2)$
- âœ… Long-range dependencies sem atenÃ§Ã£o quadrÃ¡tica
- âœ… EficiÃªncia em memÃ³ria
- âœ… Treinamento paralelo

### Time Mixing vs Channel Mixing

- **Time Mixing**: Processa ao longo da sequÃªncia (temporal)
- **Channel Mixing**: Processa ao longo das dimensÃµes (feedforward)

Combinados formam um bloco eficiente equivalente a Transformer.

### BPE Tokenization

Byte Pair Encoding:
1. ComeÃ§a com bytes individuais
2. Iterativamente funde pares mais frequentes
3. AtÃ© atingir tamanho de vocabulÃ¡rio desejado

**Vantagens:**
- Suporta qualquer caractere
- VocabulÃ¡rio compacto
- Subword units

---

## ğŸ“ˆ Status de Treinamento

**Status Atual:** Janeiro 2026 - v3 em progresso (45k steps concluÃ­dos)

### Checkpoints DisponÃ­veis

| VersÃ£o | Ãšltima | Steps | Status | Dados |
|--------|--------|-------|--------|-------|
| **v1** | checkpoint_60000 | 60k | âœ… Finalizado | Wikipedia v1 |
| **v2** | checkpoint_30000 | 30k | â¸ï¸ Pausado | Wikipedia v2 |
| **v3** | checkpoint_45000 | 45k | ğŸ”„ Ativo | Wiki + Planalto + Wikisource |
| **v12** | checkpoint_30000 | 30k | ğŸ“¦ Archive | Corpus diversificado |
| **v12_micro** | checkpoint_5000 | 5k | ğŸ“¦ Archive | Modelo micro experimental |

### Dados e Tokenizers Treinados

| VersÃ£o | Tokenizer | Dataset | Corpora |
|--------|-----------|---------|---------|
| **v2** | âœ… 32k vocab | tokenized_v2 | Wikipedia pt-br |
| **v3** | âœ… 32k vocab | tokenized_v3 | Wiki + Planalto + Wikisource |
| **v12** | âœ… 32k vocab | tokenized_v12 | Multi-corpora |
| **v15** | âœ… 32k vocab | tokenized_v15 | Ultra-clean |

### Corpora DisponÃ­veis

- âœ… Wikipedia (wiki_clean, wiki_processed_v2)
- âœ… Planalto (legislaÃ§Ã£o e atos do governo)
- âœ… Wikisource (obras literÃ¡rias)
- âœ… Wikibooks, Wikinews (conteÃºdo adicional)
- âœ… Corpus Soberano (v15 ultra-limpo)

### PrÃ³ximos Passos

- [x] Dados preparados: Wikipedia (6 corpora) + Planalto + Wikisource
- [x] v3 em treinamento (45k steps)
- [ ] Continuar v3 training atÃ© 100k steps
- [ ] Avaliar qualidade v3 vs v2
- [ ] Fine-tuning em domÃ­nios especÃ­ficos (jurÃ­dico, tÃ©cnico)
- [ ] Benchmark de performance contra baselines
- [ ] OtimizaÃ§Ãµes de quantizaÃ§Ã£o para deployment

---

## ğŸ” Notas TÃ©cnicas

### Backend Burn

Usando **NdArray** (CPU) em vez de GPU:
- Compatibilidade universal
- Sem dependÃªncias de drivers
- Ideal para desenvolvimento
- Pode ser trocado para GPU posteriormente

### Memory-Mapping

Dataset usa `memmap2` para:
- NÃ£o carregar arquivo inteiro na RAM
- Acesso rÃ¡pido a qualquer parte
- Escalabilidade

### Autodiff

Burn + Autodiff providencia:
- Automatic differentiation
- Backward pass automÃ¡tico
- Gradientes computados eficientemente

### Trainer - ImplementaÃ§Ã£o Detalhada

**InicializaÃ§Ã£o do Otimizador:**
```rust
let optimizer = AdamWConfig::new()
    .with_weight_decay(train_config.weight_decay as f32)
    .init();
```
- Otimizador criado **UMA ÃšNICA VEZ** no construtor
- Configurado com weight decay para regularizaÃ§Ã£o
- NÃ£o Ã© recreado a cada step

**Update de Pesos no Train Step:**
```rust
let grad_params = GradientsParams::from_grads(grads, &self.model);
self.model = self.optimizer.step(lr, self.model.clone(), grad_params);
```
- Extrai gradientes com `loss.backward()`
- Converte em `GradientsParams` com referÃªncia ao modelo
- `optimizer.step()` retorna modelo atualizado
- Learning rate Ã© dinÃ¢mico (warmup + decay)

**Learning Rate Schedule:**
- Fase Warmup: Aumenta linearmente de 0 atÃ© `learning_rate` durante `warmup_steps`
- Fase Decay: Reduz linearmente de `learning_rate` atÃ© 10% durante `max_steps`
- Implementado sem callback - calculado a cada step

**Loss Calculation:**
```rust
let log_probs = activation::log_softmax(logits_flat, 1);
selected.mean().neg()
```
- Cross-entropy = negativo da mean log-likelihood
- Aplicado ao vocabulÃ¡rio inteiro
- Reduzido via mÃ©dia (nÃ£o soma)

---

## ï¿½ Scripts Python (`scripts/`)

Complementam o projeto com utilitÃ¡rios de processamento de dados em Python.

### Scripts de Corpus v15 (Ultra-Clean)

#### `audit_sources_v15.py`
- **PropÃ³sito:** Auditoria de fontes de dados v15
- **Entrada:** DiretÃ³rios de corpus brutos
- **SaÃ­da:** RelatÃ³rio de qualidade, estatÃ­sticas por fonte
- **Uso:** `python scripts/audit_sources_v15.py --input data/raw`

#### `build_corpus_v15_stream.py`
- **PropÃ³sito:** Build streaming de corpus v15 (sem carregar tudo na RAM)
- **Entrada:** MÃºltiplas fontes (Wikipedia, Planalto, Wikisource)
- **SaÃ­da:** Arquivo unificado em v15_clean/
- **OtimizaÃ§Ãµes:** Iterator lazy, processamento em chunks
- **Uso:** `python scripts/build_corpus_v15_stream.py --output data/v15_clean`

#### `build_corpus_v15_v2_clean.py`
- **PropÃ³sito:** VariaÃ§Ã£o alternativa do build v15
- **DiferenÃ§a:** Diferentes critÃ©rios de limpeza, mais agressivo
- **Output:** v15_clean_v2/ (archive)

#### `clean_sources_v15.py`
- **PropÃ³sito:** Limpeza de markup para corpus v15
- **Entrada:** Arquivos brutos com markup Wikipedia/Planalto
- **SaÃ­da:** Texto limpo e normalizado
- **Remove:** HTML, templates, referÃªncias, caracteres especiais
- **Uso:** `python scripts/clean_sources_v15.py --input raw/ --output clean/`

### Scripts de Filtro (EvoluÃ§Ãµes v11-v14)

#### `filter_v11_brasil_only.py`
- **PropÃ³sito:** Filtro v11 - apenas conteÃºdo Brasil
- **CritÃ©rio:** Remove conteÃºdo nÃ£o-relacionado ao Brasil
- **Status:** âœ… Legacy/Archive
- **Output:** filtered_v11/ (archive)

#### `filter_v12_ultra_clean.py`
- **PropÃ³sito:** Filtro v12 - ultra limpeza agressiva
- **CritÃ©rio:** Muito agressivo, remove muito markup e conteÃºdo curto
- **Status:** âœ… Usado em v12 (archive)
- **Output:** filtered_v12/ (archive)

#### `filter_v13_keep_more.py`
- **PropÃ³sito:** Filtro v13 - mantÃ©m mais conteÃºdo
- **CritÃ©rio:** Menos agressivo que v12, balanÃ§o qualidade/quantidade
- **Status:** ğŸ”„ Experimental
- **Output:** filtered_v13/ (archive)

#### `filter_v14_quick_clean.py`
- **PropÃ³sito:** Filtro v14 - rÃ¡pido e pragmÃ¡tico
- **CritÃ©rio:** RÃ¡pido de executar, qualidade razoÃ¡vel
- **Status:** ğŸ”„ Experimental
- **Output:** filtered_v14/ (archive)

### Scripts de NormalizaÃ§Ã£o

#### `fix_newlines_to_lf.py`
- **PropÃ³sito:** Normalizar quebras de linha
- **Converte:** \r\n, \r â†’ \n (LF Unix standard)
- **Entrada:** Arquivos com quebras inconsistentes
- **SaÃ­da:** Arquivos com LF padronizado
- **Uso:** `python scripts/fix_newlines_to_lf.py --input raw/ --output fixed/`

### Scripts do Planalto (LegislaÃ§Ã£o Brasileira)

#### `scrape_planalto_seeds.py`
- **PropÃ³sito:** Scraper inicial de planalto.gov.br
- **Alvo:** Leis, decretos, atos administrativos, CÃ³digo Civil
- **Output:** Seeds/URLs para processamento posterior
- **Status:** âœ… v1 (legacy, funcional)

#### `scrape_planalto_seeds_v2.py`
- **PropÃ³sito:** Scraper v2 com melhorias
- **Melhorias:** 
  - Tratamento de frames HTML
  - SessÃµes HTTP com retry
  - Parser robusto de elementos
- **Output:** planalto_raw/ (bruto com markup)
- **Status:** âœ… v2 (mais robusta)

#### `scrape_planalto_fix_frames.py`
- **PropÃ³sito:** Scraper com suporte especial para frames
- **Foco:** Wikipedia-style frame extraction
- **Output:** planalto_frames_raw/
- **Status:** ğŸ”„ VariaÃ§Ã£o experimental

#### `planalto_resolve_codigo_civil.py`
- **PropÃ³sito:** Processa especificamente CÃ³digo Civil brasileiro
- **Entrada:** HTML bruto do planalto.gov.br
- **SaÃ­da:** Texto limpo com estrutura de artigos
- **Preserva:** NÃºmeros de artigos, estrutura legal
- **Status:** âœ… v1 (funcional)

#### `planalto_resolve_codigo_civil_v2.py`
- **PropÃ³sito:** VersÃ£o v2 com parsing melhorado
- **Melhoria:** 
  - MantÃ©m estrutura de artigos e incisos
  - Preserva formataÃ§Ã£o jurÃ­dica importante
  - Melhor tratamento de parÃ¡grafos
- **Output:** planalto_clean/ (como usado em v3)
- **Status:** âœ… v2 (mais preciso)

#### `planalto_codigo_civil_v3.py`
- **PropÃ³sito:** VersÃ£o v3 com suporte a mÃºltiplas leis
- **ExpansÃ£o:** NÃ£o sÃ³ CÃ³digo Civil, mas:
  - Lei Complementar
  - CÃ³digo Penal
  - Leis Especiais
  - Decreto-Lei
- **Output:** planalto_clean/ (atualizado e expandido)
- **Status:** âœ… v3 (mais abrangente)

### Scripts de ExtraÃ§Ã£o de Features

#### `extract_chroma.py`
- **PropÃ³sito:** Extrair "chroma" (caracterÃ­sticas/features salientes)
- **FunÃ§Ã£o:** Identificar trechos mais informativos e Ãºnicos do corpus
- **Output:** corpus_highlights/ (para anÃ¡lise e curation)
- **Uso:** SeleÃ§Ã£o de exemplos para fine-tuning especÃ­fico
- **Status:** ğŸ”„ Experimental, not actively used

### Scripts de UnificaÃ§Ã£o

#### `unify_corpus.py`
- **PropÃ³sito:** Unifica mÃºltiplas fontes em corpus Ãºnico coeso
- **Entrada:** 
  - Wikipedia (wiki_clean/)
  - Planalto (planalto_clean/)
  - Wikisource (wikisource_clean/)
  - Wikibooks (wikibooks_clean/)
  - Wikinews (wikinews_clean/)
- **SaÃ­da:** Arquivo Ãºnico consolidated.txt com separadores
- **EstratÃ©gia:** 
  - MantÃ©m proporÃ§Ãµes de cada fonte
  - Adiciona marcadores de seÃ§Ã£o
  - Embaralha entre fontes para melhor mix
- **Uso:** `python scripts/unify_corpus.py --output data/unified_corpus.txt`
- **Status:** âœ… Funcional, usado em v3

#### `clean_sovereign.py`
- **PropÃ³sito:** Processa corpus "Soberano" (proprietÃ¡rio/especÃ­fico)
- **Entrada:** Corpus externo de domÃ­nio especÃ­fico (ex: textos tÃ©cnicos)
- **SaÃ­da:** sovereign_clean/ (integrado ao v15)
- **FunÃ§Ã£o:** Adicionar conteÃºdo especializado ao treinamento
- **Uso:** Melhorar performance em domÃ­nios especÃ­ficos
- **Status:** âœ… Funcional, usado em v15

---

## ğŸ“‚ Estrutura de Dados Completa (`data/`)

### DiretÃ³rios de Dumps Originais

#### `dumps/`
- **ConteÃºdo:** Wikipedia XML BZ2 comprimidos
- **Arquivos:** 3 arquivos
- **Tamanho Total:** 130.06 MB
- **Formato:** BZ2 (comprimido)
- **Descomprimido:** ~600 MB-1 GB (estimado)
- **Uso:** Entrada para `process-wiki` command
- **Status:** âœ… DisponÃ­vel

#### `raw/`
- **ConteÃºdo:** Dados brutos antes de processamento
- **Fontes:** Scrapes do Planalto, exports de Wikibooks, etc
- **Formato:** Texto/HTML misto
- **Tamanho:** 2.4 GB (1 arquivo)
- **Arquivos:** Corpus bruto nÃ£o-processado
- **Status:** âš ï¸ IntermediÃ¡rio (descartÃ¡vel apÃ³s limpeza)

### DiretÃ³rios Limpos (Processados)

#### `wiki_clean/`
- **ConteÃºdo:** Wikipedia PT-BR apÃ³s WikiCleaner
- **Arquivos:** 132 arquivos
- **Tamanho Total:** 2.38 GB
- **Formato:** TXT (um documento por linha ou por arquivo)
- **Estrutura:** `wiki_000.txt`, `wiki_001.txt`, etc (10k artigos cada)
- **Usado Por:** `train-tokenizer`, `tokenize` commands
- **Status:** âœ… Ativo, qualidade alta

#### `wiki_processed_v2/`
- **ConteÃºdo:** Wikipedia processada versÃ£o 2
- **Arquivos:** 0 (diretÃ³rio vazio)
- **Tamanho Total:** 0 MB
- **Status:** ğŸ“¦ DiretÃ³rio vazio/descartado

#### `planalto_clean/`
- **ConteÃºdo:** LegislaÃ§Ã£o brasileira (CÃ³digo Civil, leis, decretos)
- **Arquivos:** 18 arquivos
- **Tamanho Total:** 4.74 MB
- **Fonte:** planalto.gov.br scraped via scripts Python
- **ConteÃºdo Detalhado:**
  - `CDC.txt` (86 KB) - CÃ³digo de Defesa do Consumidor
  - `CLT.txt` (1.4 MB) - ConsolidaÃ§Ã£o das Leis do Trabalho
  - `CODIGO_CIVIL.txt` (27 KB) - CÃ³digo Civil Brasileiro
  - `CODIGO_PENAL.txt` (265 KB) - CÃ³digo Penal
  - `CONSTITUICAO_FEDERAL.txt` (63 KB) - ConstituiÃ§Ã£o Federal 1988
  - `CPC.txt` (608 KB) - CÃ³digo de Processo Civil
  - `CPP.txt` (394 KB) - CÃ³digo de Processo Penal
  - `CTN.txt` (105 KB) - CÃ³digo TributÃ¡rio Nacional
  - `LEI_ANTICORRUPCAO.txt` (35 KB) - Lei de AnticorrupÃ§Ã£o
  - `LEI_FALENCIAS.txt` (270 KB) - Lei de FalÃªncias e RecuperaÃ§Ã£o
  - `LEI_INQUILINATO.txt` (63 KB) - Lei do Inquilinato
  - `LEI_LICITACOES_1993.txt` (209 KB) - Lei de LicitaÃ§Ãµes 1993
  - `LGPD.txt` (110 KB) - Lei Geral de ProteÃ§Ã£o de Dados
  - `MARCO_CIVIL_INTERNET.txt` (45 KB) - Marco Civil da Internet
  - `NOVA_LEI_LICITACOES.txt` (279 KB) - Nova Lei de LicitaÃ§Ãµes 2021
  - `CODIGO_CIVIL.html` (976 KB) - Backup em HTML
  - `CODIGO_CIVIL_best_attempt.txt` (387 B) - Debug/tentativa
  - `CODIGO_CIVIL_debug.txt` (387 B) - Debug/tentativa
- **Estrutura:** Preserva artigos, incisos, parÃ¡grafos numerados
- **Qualidade:** Alta, textos oficiais brasileiros
- **Usado Em:** v3, v15 training
- **Status:** âœ… Ativo, base jurÃ­dica completa

#### `wikisource_clean/`
- **ConteÃºdo:** Obras literÃ¡rias de domÃ­nio pÃºblico
- **Arquivos:** 11 arquivos
- **Tamanho Total:** 178.47 MB
- **Fonte:** Wikisource PT-BR
- **Exemplos:** Machado de Assis, AluÃ­sio Azevedo, Cruz e Sousa, etc
- **Linguagem:** PortuguÃªs clÃ¡ssico + moderno
- **Usado Em:** v3, v15 training
- **Status:** âœ… Ativo, qualidade literÃ¡ria

#### `wikibooks_clean/`
- **ConteÃºdo:** ConteÃºdo educacional (manuais, tutoriais)
- **Arquivos:** 2 arquivos
- **Tamanho Total:** 29.68 MB
- **Fonte:** Wikibooks PT-BR
- **TÃ³picos:** ProgramaÃ§Ã£o, matemÃ¡tica, histÃ³ria, idiomas, etc
- **Usado Em:** v3, v15 training
- **Status:** âœ… Ativo, conteÃºdo tÃ©cnico

#### `wikinews_clean/`
- **ConteÃºdo:** NotÃ­cias de arquivo (2005-2020)
- **Arquivos:** 4 arquivos
- **Tamanho Total:** 57.36 MB
- **Fonte:** Wikinews PT-BR
- **Utilidade:** Linguagem natural contemporÃ¢nea, eventos histÃ³ricos, atualidades
- **Usado Em:** v3, v15 training
- **Status:** âœ… Ativo, linguagem atual

#### `v15_clean/`
- **ConteÃºdo:** Corpus ultra-limpo unificado v15
- **Arquivos:** 33 arquivos
- **Tamanho Total:** 1.2 GB
- **Fontes:** Wikipedia + Planalto + Wikisource + Wikibooks + Wikinews
- **Qualidade:** AltÃ­ssima (muito filtrado, agressivo)
- **CaracterÃ­sticas:** 
  - RemoÃ§Ã£o agressiva de stubs
  - Filtro por comprimento mÃ­nimo
  - DeduplicaÃ§Ã£o aplicada
  - NormalizaÃ§Ã£o intensa
- **Usado Em:** v15 training (mais recente)
- **Status:** âœ… Ativo, melhor qualidade conhecida

### DiretÃ³rios de TokenizaÃ§Ã£o

#### `tokenizer/` (legacy)
- **ConteÃºdo:** VocabulÃ¡rio antigo (v1/v2)
- **Arquivos:** 0 (diretÃ³rio vazio)
- **Tamanho:** 0 MB
- **Status:** ğŸ“¦ Descartado (diretÃ³rio vazio)

#### `tokenizer_v2/`
- **ConteÃºdo:** VocabulÃ¡rio BPE 32k para v2
- **Arquivo:** `tokenizer.json`
- **Tamanho:** 1.25 MB
- **Vocab Size:** 32.000 tokens BPE
- **Baseado Em:** Wikipedia v2 limpa
- **Treinado Com:** build_corpus_v2 dataset
- **Status:** âœ… Funcional, legacy

#### `tokenizer_v3/`
- **ConteÃºdo:** VocabulÃ¡rio BPE 32k para v3
- **Arquivo:** `tokenizer.json`
- **Tamanho:** 1.31 MB
- **Vocab Size:** 32.000 tokens BPE
- **Baseado Em:** Wikipedia + Planalto + Wikisource + Wikibooks
- **Treinado Com:** build_corpus_v3 (multi-source)
- **Status:** âœ… Funcional, ativo

#### `tokenizer_v12/`
- **ConteÃºdo:** VocabulÃ¡rio BPE 32k para v12
- **Arquivo:** `tokenizer.json`
- **Tamanho:** 1.33 MB
- **Vocab Size:** 32.000 tokens BPE
- **Baseado Em:** Multi-corpus experimental
- **Status:** ğŸ“¦ Archive (experimental)

#### `tokenizer_v15/`
- **ConteÃºdo:** VocabulÃ¡rio BPE 32k para v15 (ultra-clean)
- **Arquivo:** `tokenizer.json`
- **Tamanho:** 1.27 MB
- **Vocab Size:** 32.000 tokens BPE
- **Baseado Em:** v15_clean (ultra-clean, multi-source)
- **Treinado Com:** build_corpus_v15 (mais rigoroso)
- **Qualidade:** Mais especÃ­fico, menos ruÃ­do
- **Status:** âœ… Funcional, mais recente

### DiretÃ³rios de Dataset Tokenizado

#### `tokenized_v2/`
- **ConteÃºdo:** Dataset tokenizado para v2
- **Arquivo:** `train.bin`
- **Formato:** BinÃ¡rio (tokens u16, little-endian)
- **Tamanho:** 1.10 GB
- **Tokens Aproximados:** ~550M tokens (estimado)
- **Seq Len:** 256-512
- **Status:** âœ… Funcional, legacy

#### `tokenized_v3/`
- **ConteÃºdo:** Dataset tokenizado para v3
- **Arquivo:** `train.bin`
- **Formato:** BinÃ¡rio (tokens u16, little-endian)
- **Tamanho:** 88.63 MB
- **Tokens Aproximados:** ~44M tokens (estimado)
- **Seq Len:** 256-2048
- **Status:** âœ… Funcional, em uso

#### `tokenized_v12/`
- **ConteÃºdo:** Dataset tokenizado para v12
- **Arquivo:** `train.bin`
- **Tamanho:** 39.04 MB
- **Status:** ğŸ“¦ Archive (experimental)

#### `tokenized_v15/`
- **ConteÃºdo:** Dataset tokenizado para v15 (ultra-clean)
- **Arquivo:** `train.bin`
- **Formato:** BinÃ¡rio (tokens u16, little-endian)
- **Tamanho:** 555.39 MB
- **Tokens Aproximados:** ~277M tokens (estimado)
- **Qualidade:** AltÃ­ssima (muito filtrado, deduplicated)
- **Status:** âœ… Funcional, mais recente

### DiretÃ³rios Processados (Legacy)

#### `processed/`
- **SubdiretÃ³rios:**
  - `corpus/` - Wikipedia parseada antiga (132 arquivos, 3.88 GB)
  - `tokenized/` - Dataset antigo (legacy)
- **Tamanho Total:** 3.88 GB
- **Status:** ğŸ“¦ Archive/Backup

### Corpus ProprietÃ¡rio

#### `sovereign/`
- **ConteÃºdo:** Corpus proprietÃ¡rio com mÃºltiplas versÃµes acumuladas
- **Tamanho Total:** 7.6 GB (10 arquivos)
- **PropÃ³sito:** Acumular versÃµes experimentais para comparaÃ§Ã£o e anÃ¡lise
- **Arquivos Detalhados:**
  - `corpus_v3.txt` (1.88 GB) - VersÃ£o v3 original, base histÃ³rica
  - `corpus_v15_base.txt` (1.17 GB) - VersÃ£o v15 preparada para treinamento
  - `corpus_v14_clean.txt` (968 MB) - VersÃ£o v14 limpa
  - `corpus_v13_generous.txt` (1.05 GB) - VersÃ£o v13 mais permissiva/menos agressiva
  - `corpus_v11_brasil.txt` (273 MB) - VersÃ£o v11, foco Brasil only
  - `wiki_brasil.txt` (1.92 GB) - Wikipedia Brasil limpo unificado
  - `corpus_v12_ultra.txt` (88 MB) - VersÃ£o v12 ultra-limpada (muito pequeno)
  - `corpus_sample.txt` (196 MB) - Amostra para testes e validaÃ§Ãµes
  - `leis.txt` (2.5 MB) - Textos jurÃ­dicos especializados
  - `dedup_v15.sqlite` (3.6 MB) - Database SQLite de deduplicaÃ§Ã£o v15
- **EstratÃ©gia:** Cada versÃ£o reflete diferentes limites de limpeza/filtro agressivo
- **Uso:** Experimentos v11-v15, anÃ¡lise de qualidade de corpus, comparaÃ§Ã£o entre versÃµes
- **Status:** âœ… Ativo, mÃºltiplas versÃµes para anÃ¡lise e treinamento

---

## ğŸ“Š Resumo Completo de Arquivos

### Arquivos Rust (`src/`)

| Arquivo | LOC | Componente | Status |
|---------|-----|-----------|--------|
| main.rs | 799 | CLI, funÃ§Ãµes principais | âœ… |
| model/config.rs | 124 | RWKVConfig, TrainingConfig | âœ… |
| model/rwkv.rs | 400+ | RWKV, RWKVBlock, TimeMixing, ChannelMixing | âœ… |
| model/trainer.rs | 131 | Trainer, train_step, loss | âœ… |
| model/adapters.rs | 50+ | OptimizerAdaptor | âœ… |
| model/mod.rs | 10 | Re-exports | âœ… |
| data/wiki_parser.rs | 200+ | WikiStreamParser, WikiArticle | âœ… |
| data/cleaner.rs | 150+ | WikiCleaner, regex patterns | âœ… |
| data/dataset.rs | 155 | MmapDataset, DataLoader, Writer | âœ… |
| data/mod.rs | 10 | Re-exports | âœ… |
| tokenizer/bpe.rs | 500+ | BPETokenizer, BPETrainer, BPEVocab | âœ… |
| tokenizer/normalize.rs | 100+ | PTBRNormalizer | âœ… |
| tokenizer/mod.rs | 10 | Re-exports | âœ… |
| **TOTAL** | **~2700** | **13 arquivos** | **âœ… Completo** |

### Scripts Python (`scripts/`)

| Script | PropÃ³sito | VersÃ£o | Status |
|--------|-----------|--------|--------|
| audit_sources_v15.py | Auditoria de fontes | v15 | âœ… |
| build_corpus_v15_stream.py | Build corpus v15 | v15 | âœ… |
| build_corpus_v15_v2_clean.py | Build alternativo v15 | v15 | ğŸ”„ |
| clean_sources_v15.py | Limpeza v15 | v15 | âœ… |
| clean_sovereign.py | Corpus soberano | - | âœ… |
| extract_chroma.py | ExtraÃ§Ã£o de features | - | ğŸ”„ |
| filter_v11_brasil_only.py | Filtro Brasil | v11 | ğŸ“¦ |
| filter_v12_ultra_clean.py | Filtro ultra-clean | v12 | ğŸ“¦ |
| filter_v13_keep_more.py | Filtro menos agressivo | v13 | ğŸ”„ |
| filter_v14_quick_clean.py | Filtro rÃ¡pido | v14 | ğŸ”„ |
| fix_newlines_to_lf.py | Normalizar quebras | - | âœ… |
| planalto_codigo_civil_v3.py | LegislaÃ§Ã£o v3 | v3 | âœ… |
| planalto_resolve_codigo_civil.py | CC parsing v1 | v1 | ğŸ“¦ |
| planalto_resolve_codigo_civil_v2.py | CC parsing v2 | v2 | âœ… |
| scrape_planalto_fix_frames.py | Scraper frames | - | ğŸ”„ |
| scrape_planalto_seeds.py | Scraper v1 | v1 | ğŸ“¦ |
| scrape_planalto_seeds_v2.py | Scraper v2 | v2 | âœ… |
| unify_corpus.py | Unificar fontes | - | âœ… |
| **TOTAL** | **18 scripts** | **Multi-versÃ£o** | **âœ… Completo** |

### Estrutura de Dados - Resumo Completo

| DiretÃ³rio | Arquivos | Tamanho | Tipo | Status |
|-----------|----------|--------|------|--------|
| dumps/ | 3 | 130.06 MB | Wikipedia BZ2 | âœ… Ativo |
| raw/ | 1 | 2.4 GB | Dados brutos | âš ï¸ IntermediÃ¡rio |
| wiki_clean/ | 132 | 2.38 GB | Wikipedia TXT | âœ… Ativo |
| wiki_processed_v2/ | 0 | 0 MB | - | ğŸ“¦ Vazio |
| planalto_clean/ | 18 | 4.74 MB | LegislaÃ§Ã£o | âœ… Ativo |
| wikisource_clean/ | 11 | 178.47 MB | Literatura | âœ… Ativo |
| wikibooks_clean/ | 2 | 29.68 MB | Educacional | âœ… Ativo |
| wikinews_clean/ | 4 | 57.36 MB | NotÃ­cias | âœ… Ativo |
| v15_clean/ | 33 | 1.2 GB | Ultra-clean | âœ… Ativo |
| processed/ | 132 | 3.88 GB | Legacy | ğŸ“¦ Archive |
| tokenizer/ | 0 | 0 MB | - | ğŸ“¦ Vazio |
| tokenizer_v2/ | 1 | 1.25 MB | BPE Vocab | âœ… Funcional |
| tokenizer_v3/ | 1 | 1.31 MB | BPE Vocab | âœ… Ativo |
| tokenizer_v12/ | 1 | 1.33 MB | BPE Vocab | ğŸ“¦ Archive |
| tokenizer_v15/ | 1 | 1.27 MB | BPE Vocab | âœ… Ativo |
| tokenized_v2/ | 1 | 1.1 GB | Dataset BIN | âœ… Funcional |
| tokenized_v3/ | 1 | 88.63 MB | Dataset BIN | âœ… Ativo |
| tokenized_v12/ | 1 | 39.04 MB | Dataset BIN | ğŸ“¦ Archive |
| tokenized_v15/ | 1 | 555.39 MB | Dataset BIN | âœ… Ativo |
| sovereign/ | 10 | 7.6 GB | Multi-version | âœ… Ativo |
| **TOTAL** | **363** | **~23 GB** | **Multi-type** | **âœ… Completo** |

---

## ï¿½ğŸ“ Autores e LicenÃ§a

- **Autor:** Caike Machado Batista Costa
- **VersÃ£o:** 0.1.0
- **Edition:** Rust 2021
- **Data AtualizaÃ§Ã£o:** Janeiro 2026

---

## ğŸš€ Timeline do Projeto

### Fase 1: Foundation (âœ… ConcluÃ­da)
- [x] Arquitetura RWKV implementada em Rust
- [x] Framework Burn integrado (NdArray backend)
- [x] CLI bÃ¡sico com 8 comandos
- [x] Dataset v1 com Wikipedia

### Fase 2: Escalabilidade (âœ… ConcluÃ­da)
- [x] Memory-mapping para datasets grandes
- [x] MÃºltiplas versÃµes de tokenizers
- [x] Processamento paralelo com Rayon
- [x] Checkpoints em v2 e v3

### Fase 3: ExpansÃ£o de Dados (âœ… ConcluÃ­da)
- [x] IntegraÃ§Ã£o Planalto (legislaÃ§Ã£o)
- [x] Wikisource (obras literÃ¡rias)
- [x] Wikibooks e Wikinews
- [x] Ultra-clean corpus v15

### Fase 4: OtimizaÃ§Ã£o (ğŸ”„ Em Progresso)
- [ ] MÃ©tricas de avaliaÃ§Ã£o
- [ ] Fine-tuning especializado
- [ ] QuantizaÃ§Ã£o e compressÃ£o
- [ ] Backend GPU (Wgpu)

### Fase 5: ProduÃ§Ã£o (â³ Planejado)
- [ ] API REST
- [ ] ContainerizaÃ§Ã£o (Docker)
- [ ] Deployment em cloud
- [ ] Benchmarks pÃºblicos
