# ğŸ” Debug: Logs Detalhados no Forward

## Problema Identificado
O treino para em "Chamando model.forward..." sem mensagens de erro, indicando que estÃ¡ travando dentro do forward pass.

## Logs Adicionados

Agora vocÃª verÃ¡ logs detalhados em cada etapa:

### 1. RWKV Forward
- âœ… Embedding
- âœ… LayerNorm prÃ©
- âœ… Cada bloco (primeiro, meio, Ãºltimo)
- âœ… LayerNorm final
- âœ… Head

### 2. RWKVBlock Forward
- âœ… LayerNorm 1
- âœ… TimeMixing
- âœ… LayerNorm 2
- âœ… ChannelMixing

### 3. TimeMixing Forward
- âœ… Token shift
- âœ… Mixing
- âœ… Projections
- âœ… **WKV (pode demorar muito na primeira vez!)**
- âœ… Output

## âš ï¸ IMPORTANTE: WKV Pode Demorar Muito!

O **WKV (Weighted Key-Value)** Ã© uma operaÃ§Ã£o complexa que:
- Precisa compilar cÃ³digo CUDA JIT na primeira execuÃ§Ã£o
- Pode demorar **5-15 minutos** na primeira vez
- Ã‰ normal nÃ£o ver progresso durante esse tempo
- ApÃ³s compilar, serÃ¡ muito mais rÃ¡pido

## Como Usar no Kaggle

### 1. Atualizar CÃ³digo
```python
%cd ptbr-llm
!git pull
```

### 2. Rebuild
```python
import os
os.environ["PATH"] = f"{os.environ['HOME']}/.cargo/bin:" + os.environ["PATH"]

%cd ptbr-llm
!CARGO_BUILD_JOBS=4 cargo build --release --features cuda
```

### 3. Rodar Treino com Debug
```python
import os
os.environ["RUST_BACKTRACE"] = "full"

%cd ptbr-llm
!RUST_BACKTRACE=full ./target/release/ptbr-slm train \
  --data /kaggle/input/ptbr-v16-ready/tokenized_v16_full \
  --tokenizer /kaggle/input/ptbr-v16-ready/tokenizer_v16_full/tokenizer.json \
  --output /kaggle/working/checkpoints \
  --model-size 400m \
  --batch-size 1 \
  --grad-accum 1 \
  --seq-len 64 \
  --max-steps 1 \
  --learning-rate 3e-4 \
  --warmup-steps 1 \
  --save-every 100 \
  --eval-every 100
```

## O Que VocÃª VerÃ¡

### SequÃªncia Normal de Logs:
```
ğŸ” Debug train_step: Chamando model.forward...
ğŸ” RWKV forward: Iniciando embedding...
ğŸ” RWKV forward: Embedding completo, shape: [1, 64, 320]
ğŸ” RWKV forward: Aplicando ln_pre...
ğŸ” RWKV forward: ln_pre completo, processando 24 blocos...
ğŸ” RWKV forward: Processando bloco 1/24...
  ğŸ” Block: Aplicando ln1...
  ğŸ” Block: Chamando time_mixing.forward...
    ğŸ” TimeMixing: Token shift...
    ğŸ” TimeMixing: Mixing...
    ğŸ” TimeMixing: Projections...
    ğŸ” TimeMixing: Chamando WKV (isso pode demorar na primeira vez devido Ã  compilaÃ§Ã£o CUDA JIT)...
    [AQUI PODE DEMORAR 5-15 MINUTOS NA PRIMEIRA VEZ]
    ğŸ” TimeMixing: WKV completo!
    ğŸ” TimeMixing: Output...
  ğŸ” Block: time_mixing completo
  ğŸ” Block: Aplicando ln2...
  ğŸ” Block: Chamando channel_mixing.forward...
  ğŸ” Block: channel_mixing completo
ğŸ” RWKV forward: Processando bloco 2/24...
...
ğŸ” RWKV forward: Todos os blocos processados, aplicando ln_out...
ğŸ” RWKV forward: ln_out completo, aplicando head...
ğŸ” RWKV forward: Head completo, shape final: [1, 64, 32000]
ğŸ” Debug train_step: Forward completo, shape: [1, 64, 32000]
```

## Se Travar no WKV

Se vocÃª ver:
```
ğŸ” TimeMixing: Chamando WKV (isso pode demorar na primeira vez devido Ã  compilaÃ§Ã£o CUDA JIT)...
```

E depois nÃ£o aparecer nada por vÃ¡rios minutos, **isso Ã© normal!**

### O Que Fazer:
1. **Aguarde 10-15 minutos** - A compilaÃ§Ã£o CUDA JIT estÃ¡ rodando
2. **Monitore GPU**: Use `!nvidia-smi` em outra cÃ©lula para ver se a GPU estÃ¡ sendo usada
3. **NÃ£o interrompa** - Deixe compilar completamente

### Como Saber Se EstÃ¡ Funcionando:
- GPU deve mostrar uso de memÃ³ria (nÃ£o 0MiB)
- GPU deve mostrar Compute M. ativo
- Processo nÃ£o deve estar morto (verifique com `!ps aux | grep ptbr-slm`)

## Se Travar em Outro Lugar

Se travar em qualquer outro lugar (nÃ£o no WKV), isso indica um problema:

### Se travar em "Embedding":
- Problema com inicializaÃ§Ã£o do embedding
- Verificar se vocab_size estÃ¡ correto

### Se travar em "ln_pre":
- Problema com LayerNorm
- Verificar dimensÃµes

### Se travar em "Projections":
- Problema com operaÃ§Ãµes lineares
- Verificar memÃ³ria GPU

## Troubleshooting

### GPU NÃ£o EstÃ¡ Sendo Usada
```python
!nvidia-smi
```
Se mostrar 0MiB, pode ser problema de inicializaÃ§Ã£o CUDA.

### Processo Morreu
```python
!ps aux | grep ptbr-slm
```
Se nÃ£o aparecer nada, o processo crashou. Verifique logs completos.

### CompilaÃ§Ã£o JIT Muito Lenta
- Normal na primeira execuÃ§Ã£o
- Pode demorar atÃ© 15 minutos
- ApÃ³s compilar, serÃ¡ muito mais rÃ¡pido
- Cache serÃ¡ salvo para prÃ³ximas execuÃ§Ãµes

## PrÃ³ximos Passos

1. âœ… Atualizar cÃ³digo
2. âœ… Rebuild
3. âœ… Rodar treino mÃ­nimo
4. âœ… **AGUARDAR** durante compilaÃ§Ã£o WKV (10-15 min)
5. âœ… Verificar se completa o primeiro step
6. âœ… Se funcionar, aumentar seq-len gradualmente

## Nota Final

**PaciÃªncia Ã© fundamental!** A primeira execuÃ§Ã£o do WKV pode demorar muito devido Ã  compilaÃ§Ã£o CUDA JIT. Isso Ã© normal e esperado. ApÃ³s a primeira compilaÃ§Ã£o, serÃ¡ muito mais rÃ¡pido.
