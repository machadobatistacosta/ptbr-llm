# ğŸ““ Notebook Kaggle Completo - Copy & Paste

## ğŸ“‹ Setup Inicial (CÃ©lula 1)

```python
# Clone repositÃ³rio
!git clone https://github.com/seu-usuario/ptbr-slm.git
%cd ptbr-slm

# Verificar estrutura
!ls -la
```

---

## ğŸ”¨ Build CUDA (CÃ©lula 2) - ~15-20 min

```bash
# Instalar Rust (se necessÃ¡rio)
!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
!source $HOME/.cargo/env

# Build com CUDA
!CARGO_BUILD_JOBS=4 cargo build --release --features cuda

# Verificar binÃ¡rio
!ls -lh target/release/ptbr-slm
```

---

## âœ… VerificaÃ§Ãµes (CÃ©lula 3)

```python
import os

# Verificar GPU
!nvidia-smi

# Verificar datasets
dataset_path = "/kaggle/input/ptbr-v16-ready/tokenized_v16_full/train.bin"
tokenizer_path = "/kaggle/input/tokenizer_v16_full/tokenizer.json"

print(f"âœ… Dataset existe: {os.path.exists(dataset_path)}")
print(f"âœ… Tokenizer existe: {os.path.exists(tokenizer_path)}")

if os.path.exists(dataset_path):
    size_gb = os.path.getsize(dataset_path) / (1024**3)
    print(f"ğŸ“¦ Dataset size: {size_gb:.2f} GB")
```

---

## ğŸ§ª Teste RÃ¡pido (CÃ©lula 4)

```bash
# Teste com modelo pequeno
!./target/release/ptbr-slm info --model-size 400m
```

**Se isso funcionar, estÃ¡ tudo OK! âœ…**

---

## ğŸ‹ï¸ TREINO 400m (CÃ©lula 5)

```bash
!./target/release/ptbr-slm train \
  --data /kaggle/input/ptbr-v16-ready/tokenized_v16_full \
  --tokenizer /kaggle/input/tokenizer_v16_full/tokenizer.json \
  --output /kaggle/working/checkpoints \
  --model-size 400m \
  --batch-size 2 \
  --grad-accum 16 \
  --seq-len 256 \
  --max-steps 50000 \
  --learning-rate 3e-4 \
  --warmup-steps 500 \
  --save-every 2500 \
  --log-every 100 \
  --eval-every 1000 \
  --eval-samples 100
```

---

## ğŸ‹ï¸ TREINO 800m (CÃ©lula Alternativa)

```bash
# âš ï¸ Use seq_len menor para evitar OOM
!./target/release/ptbr-slm train \
  --data /kaggle/input/ptbr-v16-ready/tokenized_v16_full \
  --tokenizer /kaggle/input/tokenizer_v16_full/tokenizer.json \
  --output /kaggle/working/checkpoints \
  --model-size 800m \
  --batch-size 1 \
  --grad-accum 32 \
  --seq-len 128 \
  --max-steps 50000 \
  --learning-rate 3e-4 \
  --warmup-steps 500 \
  --save-every 2500 \
  --log-every 100 \
  --eval-every 1000 \
  --eval-samples 100
```

---

## ğŸ“Š Monitoramento (CÃ©lula Separada)

```python
# Monitorar GPU durante treino
!watch -n 1 nvidia-smi
```

---

## ğŸ’¾ Download Checkpoints (CÃ©lula Final)

```python
import shutil

# Comprimir checkpoint importante
checkpoint_path = "/kaggle/working/checkpoints/checkpoint_25000"
if os.path.exists(checkpoint_path):
    shutil.make_archive('checkpoint_25000', 'zip', checkpoint_path)
    print("âœ… Checkpoint comprimido!")
```

---

## âš ï¸ Notas Importantes

1. **Caminhos:** O cÃ³digo encontra `train.bin` automaticamente em `--data`
2. **Timeout:** Kaggle tem timeout de 9h - divida treinos longos
3. **EspaÃ§o:** Limite 20GB em `/kaggle/working` - delete checkpoints antigos
4. **Multi-GPU:** Por enquanto usa apenas GPU 0 (suficiente para treino)

---

## ğŸ¯ Resumo dos Caminhos

| Item | Caminho Kaggle |
|------|----------------|
| **Dataset** | `/kaggle/input/ptbr-v16-ready/tokenized_v16_full` |
| **Tokenizer** | `/kaggle/input/tokenizer_v16_full/tokenizer.json` |
| **Output** | `/kaggle/working/checkpoints` |

**CÃ³digo encontra `train.bin` automaticamente! âœ…**
