# ğŸ‡§ğŸ‡· PTBR-SLM - Contexto Completo do Projeto

**Data:** 2026-01-11  
**Linguagem:** Rust 2021  
**Framework:** Burn 0.14 (Deep Learning)  
**Arquitetura:** RWKV (Recurrent Weight Key Value)  
**Status:** Em desenvolvimento

---

## ğŸ“‹ SumÃ¡rio Executivo

Modelo de linguagem **Small Language Model (SLM)** para PortuguÃªs Brasileiro treinado do zero em Rust. Arquitetura RWKV otimizada para:
- EficiÃªncia de memÃ³ria (8GB RAM em CPU)
- Treinamento completo com dados pÃºblicos
- Inference rÃ¡pida e portÃ¡vel

**Tamanhos disponÃ­veis:** 85M, 400M, 800M, 1B, 1.5B parÃ¢metros

---

## ğŸ—ï¸ ARQUITETURA DO PROJETO

```
ptbr-slm/
â”œâ”€â”€ src/                         # CÃ³digo Rust principal
â”‚   â”œâ”€â”€ main.rs                 # CLI (1,732 linhas, 15 subcomandos)
â”‚   â”œâ”€â”€ lib.rs                  # Exports pÃºblicos
â”‚   â”œâ”€â”€ logger/                 # Sistema de logging
â”‚   â”œâ”€â”€ utils/                  # UtilitÃ¡rios (format, etc)
â”‚   â”‚
â”‚   â”œâ”€â”€ model/                  # RWKV + Treinamento
â”‚   â”‚   â”œâ”€â”€ rwkv.rs            # RWKV modelo (405 linhas)
â”‚   â”‚   â”œâ”€â”€ trainer.rs         # Training loop (213 linhas)
â”‚   â”‚   â”œâ”€â”€ config.rs          # ConfiguraÃ§Ãµes (137 linhas)
â”‚   â”‚   â”œâ”€â”€ checkpoint.rs      # Gradient checkpointing (93 linhas)
â”‚   â”‚   â”œâ”€â”€ adapters.rs        # LoRA adapters (342 linhas)
â”‚   â”‚   â”œâ”€â”€ evaluator.rs       # Eval metrics (93 linhas)
â”‚   â”‚   â”œâ”€â”€ validation.rs      # Validation callbacks (282 linhas)
â”‚   â”‚   â”œâ”€â”€ lr_finder.rs       # Learning rate finder (84 linhas)
â”‚   â”‚   â”œâ”€â”€ precision.rs       # Mixed precision (57 linhas)
â”‚   â”‚   â”œâ”€â”€ wkv_optimized.rs   # WKV otimizado (132 linhas)
â”‚   â”‚   â””â”€â”€ mod.rs             # Exports
â”‚   â”‚
â”‚   â”œâ”€â”€ tokenizer/              # BPE Tokenizer
â”‚   â”‚   â”œâ”€â”€ bpe.rs             # BPE tokenizer (454 linhas)
â”‚   â”‚   â”œâ”€â”€ normalize.rs       # PT-BR normalization (163 linhas)
â”‚   â”‚   â””â”€â”€ mod.rs             # Exports
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                    # Data loading e processamento
â”‚   â”‚   â”œâ”€â”€ dataset.rs         # MmapDataset (191 linhas)
â”‚   â”‚   â”œâ”€â”€ wiki_parser.rs     # Wikipedia XML.BZ2 parser (133 linhas)
â”‚   â”‚   â”œâ”€â”€ cleaner.rs         # Data cleaning/regex (281 linhas)
â”‚   â”‚   â””â”€â”€ mod.rs             # Exports
â”‚   â”‚
â”‚   â””â”€â”€ [5 mÃ³dulos com mod.rs]
â”‚
â”œâ”€â”€ data/                        # Dados (7.3 GB)
â”‚   â”œâ”€â”€ tokenizer_v16_full/     # Vocab + merges (JSON)
â”‚   â”œâ”€â”€ tokenized_v16_full/     # Dados tokenizados (train.bin 1.2GB)
â”‚   â”œâ”€â”€ wiki_clean/             # Wikipedia limpa (2.3 GB)
â”‚   â”œâ”€â”€ tokenizer_full_input/   # Input original (2.6 GB, 328 arquivos)
â”‚   â”œâ”€â”€ wikibooks_clean/        # WikiBooks (31 MB)
â”‚   â”œâ”€â”€ wikisource_clean/       # WikiSource (187 MB)
â”‚   â”œâ”€â”€ wikinews_clean/         # WikiNews (60 MB)
â”‚   â””â”€â”€ planalto_clean/         # LegislaÃ§Ã£o PT-BR (4 MB, 15 arquivos)
â”‚
â”œâ”€â”€ configs/                     # ConfiguraÃ§Ãµes modelo
â”‚   â””â”€â”€ model_85m.toml
â”‚
â”œâ”€â”€ scripts/                     # Pipelines de dados (Python)
â”‚   â”œâ”€â”€ build_corpus_v15_stream.py
â”‚   â”œâ”€â”€ build_v16.py
â”‚   â”œâ”€â”€ clean_sources_v15.py
â”‚   â””â”€â”€ [12+ outros scripts]
â”‚
â”œâ”€â”€ Cargo.toml                   # Manifest Rust
â”œâ”€â”€ ARQUITETURA.md               # DocumentaÃ§Ã£o tÃ©cnica
â”œâ”€â”€ README.md                    # Getting started
â””â”€â”€ LICENSE                      # Apache 2.0
```

---

## ğŸ›ï¸ ESTRUTURAS/CLASSES PRINCIPAIS (Rust)

### 1ï¸âƒ£ MODELO RWKV (`src/model/rwkv.rs`)

#### `struct RWKV<B: Backend>`
Modelo principal RWKV com N layers.

**Campos:**
```rust
pub struct RWKV<B: Backend> {
    embedding: Embedding<B>,           // Token embeddings
    ln_pre: LayerNorm<B>,              // Pre-norm
    blocks: Vec<RWKVBlock<B>>,         // N RWKVBlocks
    ln_out: LayerNorm<B>,              // Output norm
    head: Linear<B>,                   // Vocab projection
    vocab_size: usize,
    d_model: usize,
    n_layers: usize,
}
```

**MÃ©todos principais:**
- `new(config, device)` - Cria modelo novo
- `forward(input_ids) -> Tensor[batch, seq_len, vocab]` - Forward pass
- `forward_inference(input_ids) -> Tensor[batch, vocab]` - Retorna logits Ãºltimo token

#### `struct RWKVBlock<B: Backend>`
Um bloco do modelo (pre-norm + residual).

**Campos:**
```rust
pub struct RWKVBlock<B: Backend> {
    ln1: LayerNorm<B>,              // Pre-norm time mixing
    time_mixing: TimeMixing<B>,     // O(n) attention
    ln2: LayerNorm<B>,              // Pre-norm channel
    channel_mixing: ChannelMixing<B>, // FFN
    dropout: Dropout,
}
```

**Forward:**
```
x -> LN -> TimeMixing + x (residual)
  -> LN -> ChannelMixing + x (residual)
```

#### `struct TimeMixing<B: Backend>`
RNN-like attention com complexidade O(n).

**Campos:**
```rust
pub struct TimeMixing<B: Backend> {
    receptance: Linear<B>,      // Gate
    key: Linear<B>,             // Key projection
    value: Linear<B>,           // Value projection
    output: Linear<B>,          // Output projection
    
    time_decay: Param<Tensor<B, 1>>,    // Decay rate (layer-dependent)
    time_first: Param<Tensor<B, 1>>,    // Bonus primeiro token
    time_mix_k: Param<Tensor<B, 1>>,    // Mix ratio para K
    time_mix_v: Param<Tensor<B, 1>>,    // Mix ratio para V
    time_mix_r: Param<Tensor<B, 1>>,    // Mix ratio para R (gate)
    d_model: usize,
}
```

**Algoritmo WKV (Weighted Key-Value):**
- Usa WKV kernel otimizado com log-sum-exp trick
- Processa em chunks para estabilidade
- Estado: $(a, b, p)$ mantido por layer e batch

#### `struct ChannelMixing<B: Backend>`
FFN com squared ReLU.

**Campos:**
```rust
pub struct ChannelMixing<B: Backend> {
    receptance: Linear<B>,          // Gate
    key: Linear<B>,                 // ProjeÃ§Ã£o FFN
    value: Linear<B>,               // ProjeÃ§Ã£o saÃ­da
    time_mix_k: Param<Tensor<B, 1>>,
    time_mix_r: Param<Tensor<B, 1>>,
    d_model: usize,
}
```

#### `struct RWKVState<B: Backend>`
Estado para inference incremental (cache).

```rust
pub struct RWKVState<B: Backend> {
    pub time_state: Vec<(Tensor<B, 2>, Tensor<B, 2>, Tensor<B, 2>)>,
    pub channel_state: Vec<Tensor<B, 2>>,
}
```

---

### 2ï¸âƒ£ CONFIGURAÃ‡ÃƒO (`src/model/config.rs`)

#### `struct RWKVConfig`
ConfiguraÃ§Ã£o estÃ¡tica do modelo.

```rust
pub struct RWKVConfig {
    pub vocab_size: usize,          // 32k
    pub d_model: usize,             // Hidden dim (768-2304)
    pub n_layers: usize,            // 12-28
    pub d_ffn: usize,               // FFN dim
    pub max_seq_len: usize,         // Max context (512-1024)
    pub dropout: f64,
    pub layer_norm_eps: f64,
}
```

**Presets:**
- `ptbr_85m()` - d_model=768, 12 layers â†’ ~85M params
- `ptbr_400m()` - d_model=1024, 24 layers â†’ ~400M params
- `ptbr_800m()` - d_model=1536, 24 layers â†’ ~800M params
- `ptbr_1b()` - d_model=2048, 24 layers â†’ ~1B params
- `ptbr_1_5b()` - d_model=2304, 28 layers â†’ ~1.5B params

**MÃ©todos:**
- `num_parameters()` - Calcula total de parÃ¢metros
- `estimated_vram(batch_size, seq_len)` - VRAM em bytes

#### `struct TrainingConfig`
ConfiguraÃ§Ã£o de treinamento.

```rust
pub struct TrainingConfig {
    pub learning_rate: f64,                 // 3e-4
    pub batch_size: usize,                  // Batch size efetivo
    pub gradient_accumulation_steps: usize, // 16
    pub warmup_steps: usize,                // 500
    pub max_steps: usize,                   // Total steps
    pub weight_decay: f64,                  // L2 regularization
    pub gradient_clip: f64,                 // Gradient clipping
    pub save_every: usize,                  // Checkpoint interval
    pub log_every: usize,                   // Log interval
    pub min_lr_ratio: f64,                  // Min LR em cosine decay
}
```

---

### 3ï¸âƒ£ TREINAMENTO (`src/model/trainer.rs`)

#### `struct Trainer<B: AutodiffBackend>`
Training loop com gradient accumulation e LR scheduling.

**Campos:**
```rust
pub struct Trainer<B: AutodiffBackend> {
    pub model: RWKV<B>,
    optimizer: OptimizerAdaptor<AdamW<B::InnerBackend>, RWKV<B>, B>,
    config: TrainingConfig,
    model_config: RWKVConfig,
    
    // Estado
    step: usize,
    micro_step: usize,
    accumulated_loss: f32,
    
    // MÃ©tricas
    last_grad_norm: f32,
    ema_loss: f32,
    best_loss: f32,
    device: B::Device,
}
```

#### `struct TrainStats`
EstatÃ­sticas de um step.

```rust
pub struct TrainStats {
    pub loss: f32,
    pub grad_norm: f32,
    pub lr: f64,
    pub tokens_per_sec: f32,
}
```

**MÃ©todos principais:**
- `new(model_config, train_config, device)` - Cria trainer
- `train_step(input_ids, target_ids) -> Option<TrainStats>` - Micro-step (com grad accumulation)
- `get_learning_rate() -> f64` - Warmup linear + cosine decay
- `save_checkpoint(path)` - Salva modelo + metadados
- `load_checkpoint(path)` - Carrega checkpoint

**Loss:** Cross-entropy com log-softmax para estabilidade

---

### 4ï¸âƒ£ DATASET (`src/data/dataset.rs`)

#### `struct MmapDataset`
Memory-mapped dataset de tokens tokenizados (u16 little-endian).

**Campos:**
```rust
pub struct MmapDataset {
    data: Mmap,                 // File memory-mapped
    indices: Vec<usize>,        // SequÃªncia offsets
    seq_len: usize,             // Context length
    epoch: usize,               // Epoch counter
    num_tokens: usize,          // Total tokens
}
```

**MÃ©todos:**
- `from_file(path, seq_len)` - Carrega .bin
- `get(idx)` -> (Vec<u16>, Vec<u16>) - Retorna (input, target)
- `shuffle(base_seed)` - Shuffla com ChaCha8RNG
- `next_epoch()` - Incrementa epoch

#### `struct DataLoader<'a>`
Iterator para batches.

```rust
pub struct DataLoader<'a> {
    dataset: &'a MmapDataset,
    batch_size: usize,
    current_idx: usize,
}
```

**Iterator:** Retorna `(Vec<Vec<u16>>, Vec<Vec<u16>>)` - (batch inputs, batch targets)

#### `struct TokenizedDatasetWriter`
Escreve tokens em .bin.

```rust
pub struct TokenizedDatasetWriter {
    writer: BufWriter<File>,
    tokens_written: usize,
}
```

**MÃ©todos:**
- `new(path)` - Cria arquivo
- `write_tokens(&[u16])` - Escreve tokens
- `finish()` -> usize - Retorna total escrito

---

### 5ï¸âƒ£ TOKENIZER (`src/tokenizer/bpe.rs`)

#### `struct BPETokenizer`
Tokenizer BPE thread-safe com LRU cache.

**Campos:**
```rust
pub struct BPETokenizer {
    id_to_token: Vec<Vec<u8>>,
    token_to_id: HashMap<Vec<u8>, u16>,
    merges: Vec<(u16, u16)>,
    special_tokens: HashMap<String, u16>,
    cache: Arc<RwLock<LRUCache>>,  // 100k cache
}
```

**Tokens especiais:**
```rust
PAD_TOKEN = "[PAD]"
UNK_TOKEN = "[UNK]"
BOS_TOKEN = "[BOS]"
EOS_TOKEN = "[EOS]"
SEP_TOKEN = "[SEP]"
```

**MÃ©todos:**
- `from_file(path)` - Carrega tokenizer JSON
- `from_vocab(vocab)` - Cria de BPEVocab
- `encode(&str)` -> Vec<u16> - Tokeniza texto
- `encode_batch(&[String])` -> Vec<Vec<u16>> - Paralelo com Rayon
- `decode(&[u16])` -> String - Detokeniza
- `vocab_size()` -> usize

#### `struct BPEVocab`
Vocab serializado (Serde JSON).

```rust
pub struct BPEVocab {
    pub id_to_token: Vec<Vec<u8>>,
    pub merges: Vec<(u16, u16)>,
    pub special_tokens: HashMap<String, u16>,
}
```

#### `struct PTBRNormalizer`
Normalizador para PT-BR (`src/tokenizer/normalize.rs`).

**Features:**
- NFD decomposiÃ§Ã£o Unicode
- Lowercase
- Whitespace normalization
- RemoÃ§Ã£o diacrÃ­ticos opcionais

---

### 6ï¸âƒ£ DATA PROCESSING

#### `struct WikiCleaner` (`src/data/cleaner.rs`)
Limpeza de corpus com regex.

**OperaÃ§Ãµes:**
- Remove templates `{{ }}`
- Remove HTML tags `< >`
- Remove wiki links `[[ ]]`
- Remove categorias
- Remove imagens
- 10+ regex patterns

#### `struct WikiStreamParser` (`src/data/wiki_parser.rs`)
Parser de Wikipedia XML.BZ2 streaming.

**Features:**
- Suporta streaming (nÃ£o carrega tudo em memÃ³ria)
- BZ2 decompression
- XML parsing com quick-xml
- Extrai texto puro

---

### 7ï¸âƒ£ CHECKPOINTING (`src/model/checkpoint.rs`)

#### `struct CheckpointedActivation<B: Backend>`
Cache de activations para gradient checkpointing.

```rust
pub struct CheckpointedActivation<B: Backend> {
    recompute_fn: Box<dyn Fn() -> Tensor<B, 3>>,
    cached: RefCell<Option<Tensor<B, 3>>>,
}
```

#### `struct CheckpointConfig`
ConfiguraÃ§Ã£o de checkpointing (economizar memÃ³ria).

```rust
pub struct CheckpointConfig {
    pub enabled: bool,
    pub every_n_layers: usize,
    pub checkpoint_layers: Option<Vec<usize>>,
}
```

**Presets:**
- `aggressive()` - Checkpoint toda layer
- `balanced()` - Checkpoint a cada 2 layers

---

## ğŸ”— FLUXO DE DADOS

### Treinamento

```
Arquivo .bin (tokens u16)
    â†“
MmapDataset.get(idx) â†’ (input[seq_len], target[seq_len])
    â†“
DataLoader â†’ Batches[(batch_size, seq_len), (batch_size, seq_len)]
    â†“
Trainer.train_step():
    â”œâ”€ Forward: input_ids â†’ RWKV â†’ logits[B, S, V]
    â”œâ”€ Loss: CrossEntropy(logits, targets)
    â”œâ”€ Backward: compute_gradients()
    â”œâ”€ Micro-step: acumula gradientes
    â””â”€ Step (a cada grad_accum steps): optimizer.step()
    â†“
Salva checkpoint a cada save_every steps
```

### Inference

```
Texto (String)
    â†“
BPETokenizer.encode() â†’ input_ids[S]
    â†“
RWKV.forward_inference() â†’ logits[V]
    â†“
Sample from distribution â†’ next_token
    â†“
Repeat (token autoregressivo)
```

---

## ğŸ“Š DADOS DISPONÃVEIS

### Corpus (7.3 GB total)

| Fonte | LocalizaÃ§Ã£o | Tamanho | Arquivos | LicenÃ§a |
|-------|-------------|---------|----------|---------|
| **Wikipedia PT-BR** | `data/wiki_clean/` | 2.3 GB | 328 .txt | CC BY-SA |
| **Tokenizer Input** | `data/tokenizer_full_input/` | 2.6 GB | 328 .txt | Variado |
| **WikiBooks** | `data/wikibooks_clean/` | 31 MB | ? | CC BY-SA |
| **WikiSource** | `data/wikisource_clean/` | 187 MB | ? | DomÃ­nio pÃºblico |
| **WikiNews** | `data/wikinews_clean/` | 60 MB | ? | CC BY-SA |
| **LegislaÃ§Ã£o Planalto** | `data/planalto_clean/` | 4 MB | 15 .txt | DomÃ­nio pÃºblico |
| **TOKENIZED** | `data/tokenized_v16_full/` | 1.2 GB | 1 .bin | - |

### Tokenizer

| Arquivo | Local | Tamanho | Tipo |
|---------|-------|---------|------|
| tokenizer.json | `data/tokenizer_v16_full/` | 3.88 MB | BPE (JSON) |
| model_85m.toml | `configs/` | ~1 KB | Config |

### LegislaÃ§Ã£o Brasileira (Clean)

```
data/planalto_clean/
â”œâ”€â”€ CDC.txt                        # CÃ³digo de Defesa do Consumidor
â”œâ”€â”€ CLT.txt                        # ConsolidaÃ§Ã£o Leis Trabalho
â”œâ”€â”€ CODIGO_CIVIL.txt              # CÃ³digo Civil
â”œâ”€â”€ CODIGO_PENAL.txt              # CÃ³digo Penal
â”œâ”€â”€ CONSTITUICAO_FEDERAL.txt      # ConstituiÃ§Ã£o Federal
â”œâ”€â”€ CPC.txt                        # CÃ³digo Processo Civil
â”œâ”€â”€ CPP.txt                        # CÃ³digo Processo Penal
â”œâ”€â”€ CTN.txt                        # CÃ³digo TributÃ¡rio Nacional
â”œâ”€â”€ LEI_ANTICORRUPCAO.txt         # Lei AnticorrupÃ§Ã£o
â”œâ”€â”€ LEI_FALENCIAS.txt             # Lei de FalÃªncias
â”œâ”€â”€ LEI_INQUILINATO.txt           # Lei do Inquilinato
â”œâ”€â”€ LEI_LICITACOES_1993.txt       # Lei de LicitaÃ§Ãµes 1993
â”œâ”€â”€ LGPD.txt                       # Lei Geral ProteÃ§Ã£o Dados
â”œâ”€â”€ MARCO_CIVIL_INTERNET.txt      # Marco Civil da Internet
â””â”€â”€ NOVA_LEI_LICITACOES.txt       # Nova Lei LicitaÃ§Ãµes
```

---

## ğŸ› ï¸ DEPENDÃŠNCIAS (Cargo.toml)

### Principais

| Crate | VersÃ£o | FunÃ§Ã£o |
|-------|--------|--------|
| **burn** | 0.14 | Deep learning framework |
| **ndarray** | (via burn) | CPU backend |
| **cuda-jit** | (via burn) | GPU backend (NVIDIA) |
| **wgpu** | (via burn) | GPU backend (vulkan, metal, dx12) |
| **serde/serde_json** | 1.0 | SerializaÃ§Ã£o JSON |
| **memmap2** | 0.9 | Memory-mapped files |
| **bzip2** | 0.4 | DecompressÃ£o BZ2 |
| **regex** | 1.10 | Pattern matching |
| **unicode-normalization** | 0.1 | Normalizador Unicode |
| **rayon** | 1.10 | ParalelizaÃ§Ã£o |
| **clap** | 4.5 | CLI parser |
| **quick-xml** | 0.31 | XML parser |
| **tracing** | 0.1 | Logging estruturado |

### Features

```toml
[features]
default = []           # Vazio para evitar conflitos
cpu = ["burn/ndarray"]
gpu = ["burn/wgpu"]
cuda = ["burn/cuda-jit"]
```

**Build:** `cargo build --release --features cuda`

---

## ğŸ¯ CLI COMMANDS (15 subcomandos em main.rs)

```
ptbr-slm <COMMAND>

COMMANDS:
  process-wiki     Processa dump Wikipedia XML.BZ2
  train-tokenizer  Treina tokenizer BPE
  tokenize         Tokeniza corpus para binÃ¡rio
  train            Treina modelo RWKV
  resume           Retoma treino de checkpoint
  test-model       Testa modelo com prompts
  generate         Gera texto a partir de prompt
  clean-corpus     Limpa corpus de texto
  audit-corpus     Audita qualidade do corpus
  info             Mostra informaÃ§Ãµes do modelo
  build-dataset    ConstrÃ³i dataset tokenizado
  test-gpu         Testa se GPU suporta o modelo
  find-lr          Encontra learning rate Ã³timo
  benchmark        Benchmark de performance
```

### Exemplos de uso:

```bash
# Treinar modelo
./target/release/ptbr-slm train \
  --data ./data/tokenized_v16_full/train.bin \
  --tokenizer ./data/tokenizer_v16_full/tokenizer.json \
  --output ./runs/smoke_800m \
  --model-size 800m \
  --max-steps 2 \
  --batch-size 1 \
  --seq-len 64 \
  --learning-rate 3e-4

# Gerar texto
./target/release/ptbr-slm generate \
  --model ./runs/checkpoint.bin \
  --tokenizer ./tokenizer.json \
  --prompt "O Brasil Ã©" \
  --max-tokens 100
```

---

## ğŸ“ˆ STATUS ATUAL

### âœ… Implementado
- [x] Arquitetura RWKV completa
- [x] BPE tokenizer with LRU cache
- [x] Memory-mapped dataset loading
- [x] Training loop com gradient accumulation
- [x] Checkpoint save/load
- [x] Learning rate scheduling (warmup + cosine)
- [x] Wikipedia parser
- [x] Text cleaner com regex
- [x] CLI com 15 comandos
- [x] Multi-backend suporte (CPU/GPU/CUDA)
- [x] Batch encode/decode (Rayon)

### âŒ NÃ£o implementado / TODO
- [ ] Gradient clipping (aguardando Burn API)
- [ ] Quantization (INT8, FP8)
- [ ] Flash Attention optimization
- [ ] Distributed training (multi-GPU)
- [ ] Inference optimization (KV cache, batching)
- [ ] Deployment (ONNX, TorchScript)

### ğŸ”§ Build Status
- `cargo build --release` â†’ âŒ Falha (feature default vazio)
- `cargo build --release --features cuda` â†’ âœ… Sucesso

---

## ğŸ“ ESTRUTURA DE ARQUIVOS IMPORTANTES

### CÃ³digo (5,064 linhas total Rust)

**Por mÃ³dulo:**
- `model/` - 1,734 linhas (rwkv, trainer, config, etc)
- `tokenizer/` - 617 linhas (bpe, normalize)
- `data/` - 605 linhas (dataset, cleaner, parser)
- `main.rs` - 1,732 linhas (CLI dispatcher)
- `logger/`, `utils/` - 376 linhas

**Por arquivo (top 10):**
1. main.rs - 1,732
2. bpe.rs - 454
3. adapters.rs - 342
4. rwkv.rs - 405
5. validation.rs - 282
6. cleaner.rs - 281
7. metrics.rs - 264
8. trainer.rs - 213
9. dataset.rs - 191
10. normalize.rs - 163

### Dados (7.3 GB)

**Por tipo:**
- Texto limpo: 5.3 GB (328 arquivos)
- Tokenizado: 1.2 GB (1 arquivo .bin)
- Vocab + Merges: 3.88 MB (JSON)

**Por linguagem:**
- Wikipedia PT-BR: 2.3 GB
- LegislaÃ§Ã£o: 4 MB
- Diverso: 5 GB

---

## ğŸš€ PRÃ“XIMOS PASSOS SUGERIDOS

1. **Treinar modelo 85M** completo em dados reais
2. **Avaliar perplexidade** em teste set
3. **Otimizar inference** (cache, batching)
4. **QuantizaÃ§Ã£o** (reduzir tamanho modelo)
5. **Deploy** em container/web API
6. **Fine-tuning** em domÃ­nios especÃ­ficos (jurÃ­dico, etc)

---

## ğŸ“ NOTAS

- **Backend:** Burn 0.14 (desenvolvimento ativo)
- **Rust:** Edition 2021
- **CUDA:** Suportado via `--features cuda`
- **CPU:** NdArray backend como fallback
- **Dados:** 100% pÃºblicos, licenÃ§as abertas
- **VRAM:** ~8GB para 85M, ~16GB para 1B

---

## ğŸ” LicenÃ§a

Apache 2.0 - Veja LICENSE

---

**Criado por:** Caike Costa  
**Ãšltima atualizaÃ§Ã£o:** 2026-01-11
