# ğŸ‡§ğŸ‡· PTBR-LLM

**Small Language Model para PortuguÃªs Brasileiro, treinado do zero em Rust.**

[![License](https://img.shields.io/badge/license-Apache--2.0-blue.svg)](LICENSE)
[![Rust](https://img.shields.io/badge/rust-2021-orange.svg)](https://www.rust-lang.org/)
[![Burn](https://img.shields.io/badge/burn-0.14-red.svg)](https://burn.dev/)

---

## ğŸ¯ Sobre

Modelo de linguagem baseado na arquitetura **RWKV**, otimizado para:

- âœ… Rodar em mÃ¡quinas com **8GB RAM** (CPU)
- âœ… Treinar do zero com pipeline completo
- âœ… Processar texto em **PortuguÃªs Brasileiro**
- âœ… Fontes 100% pÃºblicas e licenciadas

### Modelos

| Config | ParÃ¢metros | RAM | Status |
|--------|------------|-----|--------|
| **micro** | 10M | ~2GB | âœ… Treinado |
| **mini** | 20M | ~4GB | âœ… Treinado |
| **85m** | 85M | ~8GB | ğŸ”„ Em treino |

---

## ğŸš€ Quick Start

```bash
# Compilar
cargo build --release

# Treinar (exemplo mini, 20k steps)
./target/release/ptbr-llm train \
  --data data/tokenized \
  --tokenizer data/tokenizer/tokenizer.json \
  --output checkpoints \
  --model-size mini \
  --max-steps 20000 \
  --batch-size 4 \
  --save-every 5000

# Gerar texto
./target/release/ptbr-llm generate \
  --model checkpoints/checkpoint_20000.bin \
  --tokenizer data/tokenizer/tokenizer.json \
  --prompt "O Brasil Ã©" \
  --max-tokens 50 \
  --model-size mini

---

## ğŸ“ Estrutura

```
ptbr-llm/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs           # CLI
â”‚   â”œâ”€â”€ model/            # RWKV + Trainer
â”‚   â”œâ”€â”€ data/             # Dataset + Parser
â”‚   â””â”€â”€ tokenizer/        # BPE
â”œâ”€â”€ scripts/              # Pipelines de dados
â”œâ”€â”€ Cargo.toml
â””â”€â”€ ARQUITETURA.md        # DocumentaÃ§Ã£o tÃ©cnica completa
```

---

## ğŸ”§ Flags de Treinamento

| Flag | DescriÃ§Ã£o | Default |
|------|-----------|---------|
| `--model-size` | micro, mini, 85m | mini |
| `--max-steps` | Total de steps | 50000 |
| `--batch-size` | Batch size | 2 |
| `--grad-accum` | Gradient accumulation | 16 |
| `--learning-rate` | Learning rate | 3e-4 |
| `--warmup-steps` | Warmup steps | 500 |
| `--save-every` | Checkpoint interval | 2500 |
| `--seq-len` | Sequence length | 256 |

---

## ğŸ“š Fontes de Dados

Todas pÃºblicas e com licenÃ§a compatÃ­vel:

- **Wikipedia PT-BR** (CC BY-SA)
- **Wikisource** (domÃ­nio pÃºblico)
- **Wikibooks** (CC BY-SA)
- **LegislaÃ§Ã£o brasileira** (domÃ­nio pÃºblico)

> âš ï¸ Dados nÃ£o incluÃ­dos no repositÃ³rio. Veja `scripts/` para pipelines.

---

## ğŸ‹ï¸ Treinar no Kaggle

1. Suba `tokenizer.json` + `train.bin` como Dataset
2. Clone este repo no notebook
3. Execute:

```bash
./target/release/ptbr-llm train \
  --data /kaggle/input/seu-dataset \
  --tokenizer /kaggle/input/seu-dataset/tokenizer.json \
  --output /kaggle/working/checkpoints \
  --model-size mini \
  --max-steps 20000
```

---

## ğŸ“– DocumentaÃ§Ã£o

- **[ARQUITETURA.md](ARQUITETURA.md)** - DocumentaÃ§Ã£o tÃ©cnica completa
- **[scripts/](scripts/)** - Pipelines de processamento

---

## ğŸ“„ LicenÃ§a

[Apache License 2.0](LICENSE)

---

## ğŸ‘¤ Autor

**Caike Machado Batista Costa**

---

## ğŸ”— Links

- [Burn Framework](https://burn.dev/)
- [RWKV Paper](https://arxiv.org/abs/2305.13048)
```

---
